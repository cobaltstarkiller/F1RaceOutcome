2024-07-01 19:13:00,343:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-01 19:13:00,343:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-01 19:13:00,343:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-01 19:13:00,343:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-01 19:13:01,326:INFO:PyCaret ClassificationExperiment
2024-07-01 19:13:01,326:INFO:Logging name: clf-default-name
2024-07-01 19:13:01,326:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-07-01 19:13:01,326:INFO:version 3.3.2
2024-07-01 19:13:01,326:INFO:Initializing setup()
2024-07-01 19:13:01,326:INFO:self.USI: fa3d
2024-07-01 19:13:01,326:INFO:self._variable_keys: {'y_test', 'gpu_param', 'fold_generator', 'fold_shuffle_param', 'exp_id', 'memory', 'pipeline', 'fold_groups_param', 'fix_imbalance', 'logging_param', 'n_jobs_param', 'target_param', '_ml_usecase', 'X_test', 'seed', 'USI', 'X', 'X_train', 'exp_name_log', 'log_plots_param', 'y_train', 'idx', 'data', 'gpu_n_jobs_param', 'y', 'html_param', 'is_multiclass', '_available_plots'}
2024-07-01 19:13:01,326:INFO:Checking environment
2024-07-01 19:13:01,326:INFO:python_version: 3.11.9
2024-07-01 19:13:01,326:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-07-01 19:13:01,326:INFO:machine: AMD64
2024-07-01 19:13:01,326:INFO:platform: Windows-10-10.0.19045-SP0
2024-07-01 19:13:01,326:INFO:Memory: svmem(total=17049022464, available=8707538944, percent=48.9, used=8341483520, free=8707538944)
2024-07-01 19:13:01,326:INFO:Physical Core: 2
2024-07-01 19:13:01,326:INFO:Logical Core: 4
2024-07-01 19:13:01,326:INFO:Checking libraries
2024-07-01 19:13:01,326:INFO:System:
2024-07-01 19:13:01,326:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-07-01 19:13:01,326:INFO:executable: e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Scripts\python.exe
2024-07-01 19:13:01,326:INFO:   machine: Windows-10-10.0.19045-SP0
2024-07-01 19:13:01,326:INFO:PyCaret required dependencies:
2024-07-01 19:13:01,409:INFO:                 pip: 24.0
2024-07-01 19:13:01,409:INFO:          setuptools: 70.1.1
2024-07-01 19:13:01,409:INFO:             pycaret: 3.3.2
2024-07-01 19:13:01,409:INFO:             IPython: 8.26.0
2024-07-01 19:13:01,409:INFO:          ipywidgets: 8.1.3
2024-07-01 19:13:01,409:INFO:                tqdm: 4.66.4
2024-07-01 19:13:01,409:INFO:               numpy: 1.26.4
2024-07-01 19:13:01,409:INFO:              pandas: 2.1.4
2024-07-01 19:13:01,409:INFO:              jinja2: 3.1.4
2024-07-01 19:13:01,409:INFO:               scipy: 1.11.4
2024-07-01 19:13:01,409:INFO:              joblib: 1.3.2
2024-07-01 19:13:01,409:INFO:             sklearn: 1.4.2
2024-07-01 19:13:01,409:INFO:                pyod: 2.0.1
2024-07-01 19:13:01,409:INFO:            imblearn: 0.12.3
2024-07-01 19:13:01,409:INFO:   category_encoders: 2.6.3
2024-07-01 19:13:01,409:INFO:            lightgbm: 4.4.0
2024-07-01 19:13:01,409:INFO:               numba: 0.60.0
2024-07-01 19:13:01,409:INFO:            requests: 2.32.3
2024-07-01 19:13:01,409:INFO:          matplotlib: 3.7.5
2024-07-01 19:13:01,409:INFO:          scikitplot: 0.3.7
2024-07-01 19:13:01,409:INFO:         yellowbrick: 1.5
2024-07-01 19:13:01,409:INFO:              plotly: 5.22.0
2024-07-01 19:13:01,409:INFO:    plotly-resampler: Not installed
2024-07-01 19:13:01,409:INFO:             kaleido: 0.2.1
2024-07-01 19:13:01,409:INFO:           schemdraw: 0.15
2024-07-01 19:13:01,409:INFO:         statsmodels: 0.14.2
2024-07-01 19:13:01,409:INFO:              sktime: 0.26.0
2024-07-01 19:13:01,409:INFO:               tbats: 1.1.3
2024-07-01 19:13:01,409:INFO:            pmdarima: 2.0.4
2024-07-01 19:13:01,409:INFO:              psutil: 6.0.0
2024-07-01 19:13:01,409:INFO:          markupsafe: 2.1.5
2024-07-01 19:13:01,409:INFO:             pickle5: Not installed
2024-07-01 19:13:01,409:INFO:         cloudpickle: 3.0.0
2024-07-01 19:13:01,409:INFO:         deprecation: 2.1.0
2024-07-01 19:13:01,409:INFO:              xxhash: 3.4.1
2024-07-01 19:13:01,409:INFO:           wurlitzer: Not installed
2024-07-01 19:13:01,409:INFO:PyCaret optional dependencies:
2024-07-01 19:13:01,430:INFO:                shap: Not installed
2024-07-01 19:13:01,430:INFO:           interpret: Not installed
2024-07-01 19:13:01,430:INFO:                umap: Not installed
2024-07-01 19:13:01,430:INFO:     ydata_profiling: Not installed
2024-07-01 19:13:01,430:INFO:  explainerdashboard: Not installed
2024-07-01 19:13:01,430:INFO:             autoviz: Not installed
2024-07-01 19:13:01,430:INFO:           fairlearn: Not installed
2024-07-01 19:13:01,430:INFO:          deepchecks: Not installed
2024-07-01 19:13:01,430:INFO:             xgboost: 2.1.0
2024-07-01 19:13:01,430:INFO:            catboost: 1.2.5
2024-07-01 19:13:01,430:INFO:              kmodes: Not installed
2024-07-01 19:13:01,430:INFO:             mlxtend: Not installed
2024-07-01 19:13:01,430:INFO:       statsforecast: Not installed
2024-07-01 19:13:01,430:INFO:        tune_sklearn: Not installed
2024-07-01 19:13:01,430:INFO:                 ray: Not installed
2024-07-01 19:13:01,430:INFO:            hyperopt: Not installed
2024-07-01 19:13:01,430:INFO:              optuna: Not installed
2024-07-01 19:13:01,430:INFO:               skopt: Not installed
2024-07-01 19:13:01,430:INFO:              mlflow: Not installed
2024-07-01 19:13:01,430:INFO:              gradio: Not installed
2024-07-01 19:13:01,430:INFO:             fastapi: Not installed
2024-07-01 19:13:01,430:INFO:             uvicorn: Not installed
2024-07-01 19:13:01,430:INFO:              m2cgen: Not installed
2024-07-01 19:13:01,430:INFO:           evidently: Not installed
2024-07-01 19:13:01,430:INFO:               fugue: Not installed
2024-07-01 19:13:01,430:INFO:           streamlit: Not installed
2024-07-01 19:13:01,430:INFO:             prophet: Not installed
2024-07-01 19:13:01,430:INFO:None
2024-07-01 19:13:01,430:INFO:Set up data.
2024-07-01 19:13:01,526:INFO:Set up folding strategy.
2024-07-01 19:13:01,526:INFO:Set up train/test split.
2024-07-01 19:13:01,575:INFO:Set up index.
2024-07-01 19:13:01,575:INFO:Assigning column types.
2024-07-01 19:13:01,592:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-07-01 19:13:01,645:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-01 19:13:01,645:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-07-01 19:13:01,692:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-01 19:13:01,692:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-01 19:13:02,142:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-01 19:13:02,142:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-07-01 19:13:02,158:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-01 19:13:02,175:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-01 19:13:02,175:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-07-01 19:13:02,225:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-07-01 19:13:02,258:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-01 19:13:02,258:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-01 19:13:02,308:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-07-01 19:13:02,341:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-01 19:13:02,341:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-01 19:13:02,341:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-07-01 19:13:02,425:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-01 19:13:02,441:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-01 19:13:02,525:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-01 19:13:02,525:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-01 19:13:02,525:INFO:Preparing preprocessing pipeline...
2024-07-01 19:13:02,525:INFO:Set up label encoding.
2024-07-01 19:13:02,525:INFO:Set up simple imputation.
2024-07-01 19:13:02,574:INFO:Finished creating preprocessing pipeline.
2024-07-01 19:13:02,591:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\HP\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['age', 'experience',
                                             'results_grid', 'quali_position',
                                             'average_lap_time',
                                             'average_finish',
                                             'driver_standing...
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-07-01 19:13:02,591:INFO:Creating final display dataframe.
2024-07-01 19:13:02,757:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3                Target mapping   
4           Original data shape   
5        Transformed data shape   
6   Transformed train set shape   
7    Transformed test set shape   
8               Ignore features   
9              Numeric features   
10     Rows with missing values   
11                   Preprocess   
12              Imputation type   
13           Numeric imputation   
14       Categorical imputation   
15               Fold Generator   
16                  Fold Number   
17                     CPU Jobs   
18                      Use GPU   
19               Log Experiment   
20              Experiment Name   
21                          USI   

                                                Value  
0                                                1663  
1                               results_positionOrder  
2                                          Multiclass  
3   1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7...  
4                                          (8350, 66)  
5                                          (8350, 35)  
6                                          (5845, 35)  
7                                          (2505, 35)  
8                                                  31  
9                                                  10  
10                                             100.0%  
11                                               True  
12                                             simple  
13                                               mean  
14                                               mode  
15                                    StratifiedKFold  
16                                                 10  
17                                                 -1  
18                                              False  
19                                              False  
20                                   clf-default-name  
21                                               fa3d  
2024-07-01 19:13:02,881:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-01 19:13:02,881:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-01 19:13:02,975:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-01 19:13:02,975:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-01 19:13:02,975:INFO:setup() successfully completed in 1.66s...............
2024-07-01 19:13:27,466:INFO:Initializing compare_models()
2024-07-01 19:13:27,466:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-07-01 19:13:27,466:INFO:Checking exceptions
2024-07-01 19:13:27,486:INFO:Preparing display monitor
2024-07-01 19:13:27,539:INFO:Initializing Logistic Regression
2024-07-01 19:13:27,539:INFO:Total runtime is 0.0 minutes
2024-07-01 19:13:27,544:INFO:SubProcess create_model() called ==================================
2024-07-01 19:13:27,545:INFO:Initializing create_model()
2024-07-01 19:13:27,545:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:27,546:INFO:Checking exceptions
2024-07-01 19:13:27,546:INFO:Importing libraries
2024-07-01 19:13:27,546:INFO:Copying training dataset
2024-07-01 19:13:27,603:INFO:Defining folds
2024-07-01 19:13:27,603:INFO:Declaring metric variables
2024-07-01 19:13:27,608:INFO:Importing untrained model
2024-07-01 19:13:27,619:INFO:Logistic Regression Imported successfully
2024-07-01 19:13:27,634:INFO:Starting cross validation
2024-07-01 19:13:27,634:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:32,855:WARNING:create_model() for lr raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-01 19:13:32,855:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_logistic.py", line 1201, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:32,855:INFO:Initializing create_model()
2024-07-01 19:13:32,855:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:32,855:INFO:Checking exceptions
2024-07-01 19:13:32,855:INFO:Importing libraries
2024-07-01 19:13:32,855:INFO:Copying training dataset
2024-07-01 19:13:32,877:INFO:Defining folds
2024-07-01 19:13:32,877:INFO:Declaring metric variables
2024-07-01 19:13:32,877:INFO:Importing untrained model
2024-07-01 19:13:32,893:INFO:Logistic Regression Imported successfully
2024-07-01 19:13:32,893:INFO:Starting cross validation
2024-07-01 19:13:32,904:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:33,071:ERROR:create_model() for lr raised an exception or returned all 0.0:
2024-07-01 19:13:33,071:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_logistic.py", line 1201, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_logistic.py", line 1201, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:33,071:INFO:Initializing K Neighbors Classifier
2024-07-01 19:13:33,071:INFO:Total runtime is 0.09220722119013468 minutes
2024-07-01 19:13:33,071:INFO:SubProcess create_model() called ==================================
2024-07-01 19:13:33,071:INFO:Initializing create_model()
2024-07-01 19:13:33,071:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:33,071:INFO:Checking exceptions
2024-07-01 19:13:33,071:INFO:Importing libraries
2024-07-01 19:13:33,071:INFO:Copying training dataset
2024-07-01 19:13:33,097:INFO:Defining folds
2024-07-01 19:13:33,097:INFO:Declaring metric variables
2024-07-01 19:13:33,100:INFO:Importing untrained model
2024-07-01 19:13:33,111:INFO:K Neighbors Classifier Imported successfully
2024-07-01 19:13:33,111:INFO:Starting cross validation
2024-07-01 19:13:33,111:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:33,287:WARNING:create_model() for knn raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-01 19:13:33,287:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\neighbors\_classification.py", line 238, in fit
    return self._fit(X, y)
           ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\neighbors\_base.py", line 476, in _fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
KNeighborsClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:33,287:INFO:Initializing create_model()
2024-07-01 19:13:33,287:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:33,287:INFO:Checking exceptions
2024-07-01 19:13:33,287:INFO:Importing libraries
2024-07-01 19:13:33,287:INFO:Copying training dataset
2024-07-01 19:13:33,302:INFO:Defining folds
2024-07-01 19:13:33,302:INFO:Declaring metric variables
2024-07-01 19:13:33,310:INFO:Importing untrained model
2024-07-01 19:13:33,311:INFO:K Neighbors Classifier Imported successfully
2024-07-01 19:13:33,324:INFO:Starting cross validation
2024-07-01 19:13:33,324:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:33,484:ERROR:create_model() for knn raised an exception or returned all 0.0:
2024-07-01 19:13:33,484:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\neighbors\_classification.py", line 238, in fit
    return self._fit(X, y)
           ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\neighbors\_base.py", line 476, in _fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
KNeighborsClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\neighbors\_classification.py", line 238, in fit
    return self._fit(X, y)
           ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\neighbors\_base.py", line 476, in _fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
KNeighborsClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:33,484:INFO:Initializing Naive Bayes
2024-07-01 19:13:33,484:INFO:Total runtime is 0.0990950067838033 minutes
2024-07-01 19:13:33,484:INFO:SubProcess create_model() called ==================================
2024-07-01 19:13:33,484:INFO:Initializing create_model()
2024-07-01 19:13:33,484:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:33,484:INFO:Checking exceptions
2024-07-01 19:13:33,484:INFO:Importing libraries
2024-07-01 19:13:33,484:INFO:Copying training dataset
2024-07-01 19:13:33,509:INFO:Defining folds
2024-07-01 19:13:33,509:INFO:Declaring metric variables
2024-07-01 19:13:33,514:INFO:Importing untrained model
2024-07-01 19:13:33,522:INFO:Naive Bayes Imported successfully
2024-07-01 19:13:33,526:INFO:Starting cross validation
2024-07-01 19:13:33,526:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:33,696:WARNING:create_model() for nb raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-01 19:13:33,696:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\naive_bayes.py", line 263, in fit
    return self._partial_fit(
           ^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\naive_bayes.py", line 423, in _partial_fit
    X, y = self._validate_data(X, y, reset=first_call)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
GaussianNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:33,696:INFO:Initializing create_model()
2024-07-01 19:13:33,696:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:33,696:INFO:Checking exceptions
2024-07-01 19:13:33,696:INFO:Importing libraries
2024-07-01 19:13:33,696:INFO:Copying training dataset
2024-07-01 19:13:33,710:INFO:Defining folds
2024-07-01 19:13:33,710:INFO:Declaring metric variables
2024-07-01 19:13:33,718:INFO:Importing untrained model
2024-07-01 19:13:33,724:INFO:Naive Bayes Imported successfully
2024-07-01 19:13:33,724:INFO:Starting cross validation
2024-07-01 19:13:33,724:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:33,890:ERROR:create_model() for nb raised an exception or returned all 0.0:
2024-07-01 19:13:33,890:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\naive_bayes.py", line 263, in fit
    return self._partial_fit(
           ^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\naive_bayes.py", line 423, in _partial_fit
    X, y = self._validate_data(X, y, reset=first_call)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
GaussianNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\naive_bayes.py", line 263, in fit
    return self._partial_fit(
           ^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\naive_bayes.py", line 423, in _partial_fit
    X, y = self._validate_data(X, y, reset=first_call)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
GaussianNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:33,890:INFO:Initializing Decision Tree Classifier
2024-07-01 19:13:33,890:INFO:Total runtime is 0.10584957997004191 minutes
2024-07-01 19:13:33,890:INFO:SubProcess create_model() called ==================================
2024-07-01 19:13:33,890:INFO:Initializing create_model()
2024-07-01 19:13:33,890:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:33,890:INFO:Checking exceptions
2024-07-01 19:13:33,890:INFO:Importing libraries
2024-07-01 19:13:33,890:INFO:Copying training dataset
2024-07-01 19:13:33,905:INFO:Defining folds
2024-07-01 19:13:33,905:INFO:Declaring metric variables
2024-07-01 19:13:33,905:INFO:Importing untrained model
2024-07-01 19:13:33,926:INFO:Decision Tree Classifier Imported successfully
2024-07-01 19:13:33,928:INFO:Starting cross validation
2024-07-01 19:13:33,928:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:34,310:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,310:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,320:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,320:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,320:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,320:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,330:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,330:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,330:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,330:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,330:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,342:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,690:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,690:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,690:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,690:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,700:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,700:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,700:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-07-01 19:13:34,700:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,700:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,700:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,710:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,710:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,963:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,963:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,973:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-07-01 19:13:34,973:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,973:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,973:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,984:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:34,994:INFO:Calculating mean and std
2024-07-01 19:13:34,994:INFO:Creating metrics dataframe
2024-07-01 19:13:34,994:INFO:Uploading results into container
2024-07-01 19:13:34,994:INFO:Uploading model into container now
2024-07-01 19:13:34,994:INFO:_master_model_container: 1
2024-07-01 19:13:34,994:INFO:_display_container: 2
2024-07-01 19:13:34,994:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=1663, splitter='best')
2024-07-01 19:13:34,999:INFO:create_model() successfully completed......................................
2024-07-01 19:13:35,069:INFO:SubProcess create_model() end ==================================
2024-07-01 19:13:35,069:INFO:Creating metrics dataframe
2024-07-01 19:13:35,069:INFO:Initializing SVM - Linear Kernel
2024-07-01 19:13:35,069:INFO:Total runtime is 0.12550947268803914 minutes
2024-07-01 19:13:35,085:INFO:SubProcess create_model() called ==================================
2024-07-01 19:13:35,086:INFO:Initializing create_model()
2024-07-01 19:13:35,086:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:35,087:INFO:Checking exceptions
2024-07-01 19:13:35,087:INFO:Importing libraries
2024-07-01 19:13:35,087:INFO:Copying training dataset
2024-07-01 19:13:35,091:INFO:Defining folds
2024-07-01 19:13:35,091:INFO:Declaring metric variables
2024-07-01 19:13:35,091:INFO:Importing untrained model
2024-07-01 19:13:35,113:INFO:SVM - Linear Kernel Imported successfully
2024-07-01 19:13:35,147:INFO:Starting cross validation
2024-07-01 19:13:35,149:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:35,400:WARNING:create_model() for svm raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-01 19:13:35,400:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 917, in fit
    return self._fit(
           ^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 704, in _fit
    self._partial_fit(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 596, in _partial_fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
SGDClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:35,400:INFO:Initializing create_model()
2024-07-01 19:13:35,400:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:35,400:INFO:Checking exceptions
2024-07-01 19:13:35,400:INFO:Importing libraries
2024-07-01 19:13:35,400:INFO:Copying training dataset
2024-07-01 19:13:35,415:INFO:Defining folds
2024-07-01 19:13:35,415:INFO:Declaring metric variables
2024-07-01 19:13:35,420:INFO:Importing untrained model
2024-07-01 19:13:35,424:INFO:SVM - Linear Kernel Imported successfully
2024-07-01 19:13:35,439:INFO:Starting cross validation
2024-07-01 19:13:35,441:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:35,601:ERROR:create_model() for svm raised an exception or returned all 0.0:
2024-07-01 19:13:35,601:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 917, in fit
    return self._fit(
           ^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 704, in _fit
    self._partial_fit(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 596, in _partial_fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
SGDClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 917, in fit
    return self._fit(
           ^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 704, in _fit
    self._partial_fit(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 596, in _partial_fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
SGDClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:35,601:INFO:Initializing Ridge Classifier
2024-07-01 19:13:35,601:INFO:Total runtime is 0.1343759814898173 minutes
2024-07-01 19:13:35,601:INFO:SubProcess create_model() called ==================================
2024-07-01 19:13:35,601:INFO:Initializing create_model()
2024-07-01 19:13:35,601:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:35,601:INFO:Checking exceptions
2024-07-01 19:13:35,601:INFO:Importing libraries
2024-07-01 19:13:35,601:INFO:Copying training dataset
2024-07-01 19:13:35,617:INFO:Defining folds
2024-07-01 19:13:35,617:INFO:Declaring metric variables
2024-07-01 19:13:35,617:INFO:Importing untrained model
2024-07-01 19:13:35,617:INFO:Ridge Classifier Imported successfully
2024-07-01 19:13:35,646:INFO:Starting cross validation
2024-07-01 19:13:35,646:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:35,807:WARNING:create_model() for ridge raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-01 19:13:35,807:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_ridge.py", line 1477, in fit
    X, y, sample_weight, Y = self._prepare_data(X, y, sample_weight, self.solver)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_ridge.py", line 1212, in _prepare_data
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
RidgeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:35,807:INFO:Initializing create_model()
2024-07-01 19:13:35,807:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:35,807:INFO:Checking exceptions
2024-07-01 19:13:35,807:INFO:Importing libraries
2024-07-01 19:13:35,807:INFO:Copying training dataset
2024-07-01 19:13:35,822:INFO:Defining folds
2024-07-01 19:13:35,822:INFO:Declaring metric variables
2024-07-01 19:13:35,822:INFO:Importing untrained model
2024-07-01 19:13:35,822:INFO:Ridge Classifier Imported successfully
2024-07-01 19:13:35,843:INFO:Starting cross validation
2024-07-01 19:13:35,843:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:36,006:ERROR:create_model() for ridge raised an exception or returned all 0.0:
2024-07-01 19:13:36,006:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_ridge.py", line 1477, in fit
    X, y, sample_weight, Y = self._prepare_data(X, y, sample_weight, self.solver)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_ridge.py", line 1212, in _prepare_data
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
RidgeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_ridge.py", line 1477, in fit
    X, y, sample_weight, Y = self._prepare_data(X, y, sample_weight, self.solver)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_ridge.py", line 1212, in _prepare_data
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
RidgeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:36,006:INFO:Initializing Random Forest Classifier
2024-07-01 19:13:36,006:INFO:Total runtime is 0.14112701416015624 minutes
2024-07-01 19:13:36,006:INFO:SubProcess create_model() called ==================================
2024-07-01 19:13:36,006:INFO:Initializing create_model()
2024-07-01 19:13:36,006:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:36,006:INFO:Checking exceptions
2024-07-01 19:13:36,006:INFO:Importing libraries
2024-07-01 19:13:36,006:INFO:Copying training dataset
2024-07-01 19:13:36,022:INFO:Defining folds
2024-07-01 19:13:36,022:INFO:Declaring metric variables
2024-07-01 19:13:36,022:INFO:Importing untrained model
2024-07-01 19:13:36,022:INFO:Random Forest Classifier Imported successfully
2024-07-01 19:13:36,043:INFO:Starting cross validation
2024-07-01 19:13:36,043:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:39,783:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:39,794:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:39,794:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:39,804:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:39,804:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:39,804:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:39,826:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:39,827:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:39,838:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:39,879:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:39,879:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:39,879:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:43,514:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:43,516:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:43,521:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:43,525:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:43,527:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:43,527:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:43,629:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:43,629:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:43,646:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:43,746:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:43,746:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:43,746:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:45,571:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:45,575:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:45,579:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:45,612:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:45,612:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:45,624:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:45,643:INFO:Calculating mean and std
2024-07-01 19:13:45,644:INFO:Creating metrics dataframe
2024-07-01 19:13:45,644:INFO:Uploading results into container
2024-07-01 19:13:45,644:INFO:Uploading model into container now
2024-07-01 19:13:45,644:INFO:_master_model_container: 2
2024-07-01 19:13:45,644:INFO:_display_container: 2
2024-07-01 19:13:45,644:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=1663, verbose=0,
                       warm_start=False)
2024-07-01 19:13:45,644:INFO:create_model() successfully completed......................................
2024-07-01 19:13:45,741:INFO:SubProcess create_model() end ==================================
2024-07-01 19:13:45,741:INFO:Creating metrics dataframe
2024-07-01 19:13:45,749:INFO:Initializing Quadratic Discriminant Analysis
2024-07-01 19:13:45,749:INFO:Total runtime is 0.3035019755363464 minutes
2024-07-01 19:13:45,753:INFO:SubProcess create_model() called ==================================
2024-07-01 19:13:45,753:INFO:Initializing create_model()
2024-07-01 19:13:45,753:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:45,753:INFO:Checking exceptions
2024-07-01 19:13:45,754:INFO:Importing libraries
2024-07-01 19:13:45,754:INFO:Copying training dataset
2024-07-01 19:13:45,781:INFO:Defining folds
2024-07-01 19:13:45,782:INFO:Declaring metric variables
2024-07-01 19:13:45,797:INFO:Importing untrained model
2024-07-01 19:13:45,799:INFO:Quadratic Discriminant Analysis Imported successfully
2024-07-01 19:13:45,807:INFO:Starting cross validation
2024-07-01 19:13:45,807:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:45,988:WARNING:create_model() for qda raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-01 19:13:45,988:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\discriminant_analysis.py", line 899, in fit
    X, y = self._validate_data(X, y)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
QuadraticDiscriminantAnalysis does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:45,988:INFO:Initializing create_model()
2024-07-01 19:13:45,988:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:45,988:INFO:Checking exceptions
2024-07-01 19:13:45,988:INFO:Importing libraries
2024-07-01 19:13:45,988:INFO:Copying training dataset
2024-07-01 19:13:46,004:INFO:Defining folds
2024-07-01 19:13:46,004:INFO:Declaring metric variables
2024-07-01 19:13:46,018:INFO:Importing untrained model
2024-07-01 19:13:46,026:INFO:Quadratic Discriminant Analysis Imported successfully
2024-07-01 19:13:46,042:INFO:Starting cross validation
2024-07-01 19:13:46,044:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:46,206:ERROR:create_model() for qda raised an exception or returned all 0.0:
2024-07-01 19:13:46,206:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\discriminant_analysis.py", line 899, in fit
    X, y = self._validate_data(X, y)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
QuadraticDiscriminantAnalysis does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\discriminant_analysis.py", line 899, in fit
    X, y = self._validate_data(X, y)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
QuadraticDiscriminantAnalysis does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:46,206:INFO:Initializing Ada Boost Classifier
2024-07-01 19:13:46,206:INFO:Total runtime is 0.3111254692077636 minutes
2024-07-01 19:13:46,206:INFO:SubProcess create_model() called ==================================
2024-07-01 19:13:46,206:INFO:Initializing create_model()
2024-07-01 19:13:46,206:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:46,206:INFO:Checking exceptions
2024-07-01 19:13:46,206:INFO:Importing libraries
2024-07-01 19:13:46,206:INFO:Copying training dataset
2024-07-01 19:13:46,243:INFO:Defining folds
2024-07-01 19:13:46,243:INFO:Declaring metric variables
2024-07-01 19:13:46,252:INFO:Importing untrained model
2024-07-01 19:13:46,258:INFO:Ada Boost Classifier Imported successfully
2024-07-01 19:13:46,258:INFO:Starting cross validation
2024-07-01 19:13:46,275:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:46,444:WARNING:create_model() for ada raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-01 19:13:46,444:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:46,444:INFO:Initializing create_model()
2024-07-01 19:13:46,444:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:46,444:INFO:Checking exceptions
2024-07-01 19:13:46,444:INFO:Importing libraries
2024-07-01 19:13:46,444:INFO:Copying training dataset
2024-07-01 19:13:46,475:INFO:Defining folds
2024-07-01 19:13:46,475:INFO:Declaring metric variables
2024-07-01 19:13:46,481:INFO:Importing untrained model
2024-07-01 19:13:46,492:INFO:Ada Boost Classifier Imported successfully
2024-07-01 19:13:46,497:INFO:Starting cross validation
2024-07-01 19:13:46,497:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:46,659:ERROR:create_model() for ada raised an exception or returned all 0.0:
2024-07-01 19:13:46,659:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:46,659:INFO:Initializing Gradient Boosting Classifier
2024-07-01 19:13:46,659:INFO:Total runtime is 0.3186819235483805 minutes
2024-07-01 19:13:46,659:INFO:SubProcess create_model() called ==================================
2024-07-01 19:13:46,659:INFO:Initializing create_model()
2024-07-01 19:13:46,659:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:46,659:INFO:Checking exceptions
2024-07-01 19:13:46,659:INFO:Importing libraries
2024-07-01 19:13:46,659:INFO:Copying training dataset
2024-07-01 19:13:46,696:INFO:Defining folds
2024-07-01 19:13:46,696:INFO:Declaring metric variables
2024-07-01 19:13:46,697:INFO:Importing untrained model
2024-07-01 19:13:46,697:INFO:Gradient Boosting Classifier Imported successfully
2024-07-01 19:13:46,715:INFO:Starting cross validation
2024-07-01 19:13:46,715:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:46,882:WARNING:create_model() for gbc raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-01 19:13:46,882:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_gb.py", line 659, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
GradientBoostingClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:46,882:INFO:Initializing create_model()
2024-07-01 19:13:46,882:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:46,882:INFO:Checking exceptions
2024-07-01 19:13:46,882:INFO:Importing libraries
2024-07-01 19:13:46,882:INFO:Copying training dataset
2024-07-01 19:13:46,890:INFO:Defining folds
2024-07-01 19:13:46,890:INFO:Declaring metric variables
2024-07-01 19:13:46,902:INFO:Importing untrained model
2024-07-01 19:13:46,910:INFO:Gradient Boosting Classifier Imported successfully
2024-07-01 19:13:46,924:INFO:Starting cross validation
2024-07-01 19:13:46,925:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:47,169:ERROR:create_model() for gbc raised an exception or returned all 0.0:
2024-07-01 19:13:47,169:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_gb.py", line 659, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
GradientBoostingClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_gb.py", line 659, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
GradientBoostingClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:47,169:INFO:Initializing Linear Discriminant Analysis
2024-07-01 19:13:47,169:INFO:Total runtime is 0.327169394493103 minutes
2024-07-01 19:13:47,175:INFO:SubProcess create_model() called ==================================
2024-07-01 19:13:47,175:INFO:Initializing create_model()
2024-07-01 19:13:47,175:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:47,176:INFO:Checking exceptions
2024-07-01 19:13:47,176:INFO:Importing libraries
2024-07-01 19:13:47,176:INFO:Copying training dataset
2024-07-01 19:13:47,194:INFO:Defining folds
2024-07-01 19:13:47,194:INFO:Declaring metric variables
2024-07-01 19:13:47,196:INFO:Importing untrained model
2024-07-01 19:13:47,196:INFO:Linear Discriminant Analysis Imported successfully
2024-07-01 19:13:47,214:INFO:Starting cross validation
2024-07-01 19:13:47,214:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:47,376:WARNING:create_model() for lda raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-01 19:13:47,377:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\discriminant_analysis.py", line 581, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
LinearDiscriminantAnalysis does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:47,377:INFO:Initializing create_model()
2024-07-01 19:13:47,377:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:47,377:INFO:Checking exceptions
2024-07-01 19:13:47,378:INFO:Importing libraries
2024-07-01 19:13:47,378:INFO:Copying training dataset
2024-07-01 19:13:47,390:INFO:Defining folds
2024-07-01 19:13:47,390:INFO:Declaring metric variables
2024-07-01 19:13:47,399:INFO:Importing untrained model
2024-07-01 19:13:47,409:INFO:Linear Discriminant Analysis Imported successfully
2024-07-01 19:13:47,423:INFO:Starting cross validation
2024-07-01 19:13:47,426:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:47,580:ERROR:create_model() for lda raised an exception or returned all 0.0:
2024-07-01 19:13:47,582:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\discriminant_analysis.py", line 581, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
LinearDiscriminantAnalysis does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\discriminant_analysis.py", line 581, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
LinearDiscriminantAnalysis does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:47,582:INFO:Initializing Extra Trees Classifier
2024-07-01 19:13:47,582:INFO:Total runtime is 0.3340565284093221 minutes
2024-07-01 19:13:47,585:INFO:SubProcess create_model() called ==================================
2024-07-01 19:13:47,585:INFO:Initializing create_model()
2024-07-01 19:13:47,586:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:47,586:INFO:Checking exceptions
2024-07-01 19:13:47,586:INFO:Importing libraries
2024-07-01 19:13:47,586:INFO:Copying training dataset
2024-07-01 19:13:47,589:INFO:Defining folds
2024-07-01 19:13:47,589:INFO:Declaring metric variables
2024-07-01 19:13:47,607:INFO:Importing untrained model
2024-07-01 19:13:47,607:INFO:Extra Trees Classifier Imported successfully
2024-07-01 19:13:47,632:INFO:Starting cross validation
2024-07-01 19:13:47,640:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:47,828:WARNING:create_model() for et raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-01 19:13:47,828:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_forest.py", line 377, in fit
    estimator._compute_missing_values_in_feature_mask(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\tree\_classes.py", line 214, in _compute_missing_values_in_feature_mask
    assert_all_finite(X, **common_kwargs)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 216, in assert_all_finite
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
ExtraTreesClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:47,828:INFO:Initializing create_model()
2024-07-01 19:13:47,828:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:47,828:INFO:Checking exceptions
2024-07-01 19:13:47,828:INFO:Importing libraries
2024-07-01 19:13:47,828:INFO:Copying training dataset
2024-07-01 19:13:47,856:INFO:Defining folds
2024-07-01 19:13:47,856:INFO:Declaring metric variables
2024-07-01 19:13:47,874:INFO:Importing untrained model
2024-07-01 19:13:47,874:INFO:Extra Trees Classifier Imported successfully
2024-07-01 19:13:47,902:INFO:Starting cross validation
2024-07-01 19:13:47,906:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:48,079:ERROR:create_model() for et raised an exception or returned all 0.0:
2024-07-01 19:13:48,083:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_forest.py", line 377, in fit
    estimator._compute_missing_values_in_feature_mask(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\tree\_classes.py", line 214, in _compute_missing_values_in_feature_mask
    assert_all_finite(X, **common_kwargs)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 216, in assert_all_finite
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
ExtraTreesClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_forest.py", line 377, in fit
    estimator._compute_missing_values_in_feature_mask(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\tree\_classes.py", line 214, in _compute_missing_values_in_feature_mask
    assert_all_finite(X, **common_kwargs)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 216, in assert_all_finite
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
ExtraTreesClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-01 19:13:48,083:INFO:Initializing Extreme Gradient Boosting
2024-07-01 19:13:48,083:INFO:Total runtime is 0.3424020250638326 minutes
2024-07-01 19:13:48,088:INFO:SubProcess create_model() called ==================================
2024-07-01 19:13:48,089:INFO:Initializing create_model()
2024-07-01 19:13:48,089:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:13:48,089:INFO:Checking exceptions
2024-07-01 19:13:48,089:INFO:Importing libraries
2024-07-01 19:13:48,089:INFO:Copying training dataset
2024-07-01 19:13:48,112:INFO:Defining folds
2024-07-01 19:13:48,112:INFO:Declaring metric variables
2024-07-01 19:13:48,123:INFO:Importing untrained model
2024-07-01 19:13:48,130:INFO:Extreme Gradient Boosting Imported successfully
2024-07-01 19:13:48,144:INFO:Starting cross validation
2024-07-01 19:13:48,144:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:13:54,870:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:54,871:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:54,881:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:54,881:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:54,891:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:54,891:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:54,898:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:54,898:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:54,898:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:54,914:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:54,931:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:13:54,931:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:02,041:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:02,041:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:02,064:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:02,076:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:02,082:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:02,086:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:02,141:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:02,141:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:02,151:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:02,197:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:02,200:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:02,210:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:06,969:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:06,969:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:06,979:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:06,983:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:06,984:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:06,989:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:07,000:INFO:Calculating mean and std
2024-07-01 19:14:07,000:INFO:Creating metrics dataframe
2024-07-01 19:14:07,000:INFO:Uploading results into container
2024-07-01 19:14:07,000:INFO:Uploading model into container now
2024-07-01 19:14:07,000:INFO:_master_model_container: 3
2024-07-01 19:14:07,000:INFO:_display_container: 2
2024-07-01 19:14:07,000:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-07-01 19:14:07,000:INFO:create_model() successfully completed......................................
2024-07-01 19:14:07,067:INFO:SubProcess create_model() end ==================================
2024-07-01 19:14:07,067:INFO:Creating metrics dataframe
2024-07-01 19:14:07,084:INFO:Initializing Light Gradient Boosting Machine
2024-07-01 19:14:07,085:INFO:Total runtime is 0.659090252717336 minutes
2024-07-01 19:14:07,087:INFO:SubProcess create_model() called ==================================
2024-07-01 19:14:07,087:INFO:Initializing create_model()
2024-07-01 19:14:07,087:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:14:07,087:INFO:Checking exceptions
2024-07-01 19:14:07,087:INFO:Importing libraries
2024-07-01 19:14:07,087:INFO:Copying training dataset
2024-07-01 19:14:07,118:INFO:Defining folds
2024-07-01 19:14:07,118:INFO:Declaring metric variables
2024-07-01 19:14:07,126:INFO:Importing untrained model
2024-07-01 19:14:07,136:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-01 19:14:07,143:INFO:Starting cross validation
2024-07-01 19:14:07,153:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:14:21,933:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:21,943:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:21,950:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:22,167:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:22,171:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:22,171:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:23,187:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:23,199:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:23,208:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:23,509:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:23,515:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:23,519:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:37,735:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:37,735:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:37,746:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:38,531:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:38,536:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:38,536:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:39,364:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:39,368:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:39,368:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:39,642:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:39,647:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:39,652:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:47,512:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:47,512:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:47,521:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:47,698:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:47,704:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:47,708:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:14:47,728:INFO:Calculating mean and std
2024-07-01 19:14:47,728:INFO:Creating metrics dataframe
2024-07-01 19:14:47,728:INFO:Uploading results into container
2024-07-01 19:14:47,728:INFO:Uploading model into container now
2024-07-01 19:14:47,728:INFO:_master_model_container: 4
2024-07-01 19:14:47,737:INFO:_display_container: 2
2024-07-01 19:14:47,739:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-07-01 19:14:47,739:INFO:create_model() successfully completed......................................
2024-07-01 19:14:47,853:INFO:SubProcess create_model() end ==================================
2024-07-01 19:14:47,854:INFO:Creating metrics dataframe
2024-07-01 19:14:47,854:INFO:Initializing CatBoost Classifier
2024-07-01 19:14:47,854:INFO:Total runtime is 1.3385915279388427 minutes
2024-07-01 19:14:47,875:INFO:SubProcess create_model() called ==================================
2024-07-01 19:14:47,876:INFO:Initializing create_model()
2024-07-01 19:14:47,876:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:14:47,876:INFO:Checking exceptions
2024-07-01 19:14:47,876:INFO:Importing libraries
2024-07-01 19:14:47,877:INFO:Copying training dataset
2024-07-01 19:14:47,908:INFO:Defining folds
2024-07-01 19:14:47,909:INFO:Declaring metric variables
2024-07-01 19:14:47,917:INFO:Importing untrained model
2024-07-01 19:14:47,927:INFO:CatBoost Classifier Imported successfully
2024-07-01 19:14:47,927:INFO:Starting cross validation
2024-07-01 19:14:47,927:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:22:36,860:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:22:36,865:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:22:36,883:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:22:37,407:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:22:37,411:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:22:37,419:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:22:39,268:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:22:39,273:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:22:39,277:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:22:41,892:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:22:41,900:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:22:41,904:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:31:25,938:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:31:25,942:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:31:25,945:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-07-01 19:31:25,950:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:31:26,141:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:31:26,151:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:31:26,168:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:31:26,941:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:31:26,945:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:31:26,952:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:31:32,004:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:31:32,021:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:31:32,030:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:55,579:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:55,585:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:55,590:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:55,596:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:55,600:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:55,605:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:55,619:INFO:Calculating mean and std
2024-07-01 19:35:55,625:INFO:Creating metrics dataframe
2024-07-01 19:35:55,635:INFO:Uploading results into container
2024-07-01 19:35:55,636:INFO:Uploading model into container now
2024-07-01 19:35:55,637:INFO:_master_model_container: 5
2024-07-01 19:35:55,637:INFO:_display_container: 2
2024-07-01 19:35:55,638:INFO:<catboost.core.CatBoostClassifier object at 0x000002063B4D94D0>
2024-07-01 19:35:55,638:INFO:create_model() successfully completed......................................
2024-07-01 19:35:55,773:INFO:SubProcess create_model() end ==================================
2024-07-01 19:35:55,773:INFO:Creating metrics dataframe
2024-07-01 19:35:55,793:INFO:Initializing Dummy Classifier
2024-07-01 19:35:55,793:INFO:Total runtime is 22.47090408404668 minutes
2024-07-01 19:35:55,800:INFO:SubProcess create_model() called ==================================
2024-07-01 19:35:55,801:INFO:Initializing create_model()
2024-07-01 19:35:55,801:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002063B4E3A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:35:55,801:INFO:Checking exceptions
2024-07-01 19:35:55,802:INFO:Importing libraries
2024-07-01 19:35:55,802:INFO:Copying training dataset
2024-07-01 19:35:55,845:INFO:Defining folds
2024-07-01 19:35:55,845:INFO:Declaring metric variables
2024-07-01 19:35:55,853:INFO:Importing untrained model
2024-07-01 19:35:55,867:INFO:Dummy Classifier Imported successfully
2024-07-01 19:35:55,905:INFO:Starting cross validation
2024-07-01 19:35:55,910:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:35:56,183:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,189:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,193:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-07-01 19:35:56,198:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,202:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,208:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,210:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,211:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-07-01 19:35:56,213:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,214:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,218:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,220:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-07-01 19:35:56,228:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,229:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-07-01 19:35:56,232:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,332:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,337:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,337:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,342:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-07-01 19:35:56,343:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,344:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,345:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,346:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,346:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-07-01 19:35:56,349:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,349:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,350:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,351:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-07-01 19:35:56,353:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-07-01 19:35:56,354:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,370:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,458:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,461:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,463:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,465:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-07-01 19:35:56,468:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,468:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-07-01 19:35:56,471:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:35:56,492:INFO:Calculating mean and std
2024-07-01 19:35:56,494:INFO:Creating metrics dataframe
2024-07-01 19:35:56,497:INFO:Uploading results into container
2024-07-01 19:35:56,498:INFO:Uploading model into container now
2024-07-01 19:35:56,499:INFO:_master_model_container: 6
2024-07-01 19:35:56,499:INFO:_display_container: 2
2024-07-01 19:35:56,500:INFO:DummyClassifier(constant=None, random_state=1663, strategy='prior')
2024-07-01 19:35:56,500:INFO:create_model() successfully completed......................................
2024-07-01 19:35:56,622:INFO:SubProcess create_model() end ==================================
2024-07-01 19:35:56,622:INFO:Creating metrics dataframe
2024-07-01 19:35:56,642:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-07-01 19:35:56,666:INFO:Initializing create_model()
2024-07-01 19:35:56,667:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:35:56,667:INFO:Checking exceptions
2024-07-01 19:35:56,670:INFO:Importing libraries
2024-07-01 19:35:56,672:INFO:Copying training dataset
2024-07-01 19:35:56,732:INFO:Defining folds
2024-07-01 19:35:56,732:INFO:Declaring metric variables
2024-07-01 19:35:56,732:INFO:Importing untrained model
2024-07-01 19:35:56,732:INFO:Declaring custom model
2024-07-01 19:35:56,734:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-01 19:35:56,736:INFO:Cross validation set to False
2024-07-01 19:35:56,736:INFO:Fitting Model
2024-07-01 19:35:56,810:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002516 seconds.
2024-07-01 19:35:56,810:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-07-01 19:35:56,810:INFO:[LightGBM] [Info] Total Bins 3492
2024-07-01 19:35:56,811:INFO:[LightGBM] [Info] Number of data points in the train set: 5845, number of used features: 34
2024-07-01 19:35:56,812:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 19:35:56,812:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 19:35:56,813:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:35:56,813:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:35:56,813:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:35:56,813:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 19:35:56,813:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 19:35:56,813:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 19:35:56,814:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:35:56,814:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:35:56,814:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:35:56,814:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 19:35:56,814:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:35:56,814:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:35:56,814:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:35:56,815:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:35:56,815:INFO:[LightGBM] [Info] Start training from score -3.049324
2024-07-01 19:35:56,815:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:35:56,815:INFO:[LightGBM] [Info] Start training from score -3.063870
2024-07-01 19:35:56,815:INFO:[LightGBM] [Info] Start training from score -3.060214
2024-07-01 19:35:56,815:INFO:[LightGBM] [Info] Start training from score -4.009903
2024-07-01 19:35:56,816:INFO:[LightGBM] [Info] Start training from score -4.019382
2024-07-01 19:35:56,816:INFO:[LightGBM] [Info] Start training from score -5.009780
2024-07-01 19:35:56,816:INFO:[LightGBM] [Info] Start training from score -5.009780
2024-07-01 19:35:56,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:35:56,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:01,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:01,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:01,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,619:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:02,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,060:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,234:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,547:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,557:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:03,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:36:04,765:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-07-01 19:36:04,765:INFO:create_model() successfully completed......................................
2024-07-01 19:36:04,921:INFO:_master_model_container: 6
2024-07-01 19:36:04,922:INFO:_display_container: 2
2024-07-01 19:36:04,923:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-07-01 19:36:04,923:INFO:compare_models() successfully completed......................................
2024-07-01 19:41:43,401:INFO:Initializing create_model()
2024-07-01 19:41:43,401:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 19:41:43,402:INFO:Checking exceptions
2024-07-01 19:41:43,427:INFO:Importing libraries
2024-07-01 19:41:43,427:INFO:Copying training dataset
2024-07-01 19:41:43,473:INFO:Defining folds
2024-07-01 19:41:43,473:INFO:Declaring metric variables
2024-07-01 19:41:43,482:INFO:Importing untrained model
2024-07-01 19:41:43,491:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-01 19:41:43,506:INFO:Starting cross validation
2024-07-01 19:41:43,509:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 19:42:07,559:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:07,564:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:07,573:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:07,934:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:07,938:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:07,943:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:08,531:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:08,540:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:08,545:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:10,053:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:10,060:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:10,068:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:29,199:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:29,205:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:29,212:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:30,468:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:30,474:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:30,480:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:31,242:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:31,247:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:31,251:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:32,263:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:32,268:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:32,275:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:41,282:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:41,290:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:41,299:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:42,019:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:42,025:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:42,031:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:42,046:INFO:Calculating mean and std
2024-07-01 19:42:42,048:INFO:Creating metrics dataframe
2024-07-01 19:42:42,060:INFO:Finalizing model
2024-07-01 19:42:42,123:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001251 seconds.
2024-07-01 19:42:42,123:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-07-01 19:42:42,123:INFO:[LightGBM] [Info] Total Bins 3492
2024-07-01 19:42:42,123:INFO:[LightGBM] [Info] Number of data points in the train set: 5845, number of used features: 34
2024-07-01 19:42:42,125:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 19:42:42,125:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 19:42:42,125:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:42:42,127:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:42:42,127:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:42:42,127:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 19:42:42,128:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 19:42:42,128:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 19:42:42,128:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:42:42,128:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:42:42,128:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:42:42,128:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 19:42:42,128:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:42:42,128:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:42:42,129:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:42:42,129:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:42:42,129:INFO:[LightGBM] [Info] Start training from score -3.049324
2024-07-01 19:42:42,129:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 19:42:42,129:INFO:[LightGBM] [Info] Start training from score -3.063870
2024-07-01 19:42:42,129:INFO:[LightGBM] [Info] Start training from score -3.060214
2024-07-01 19:42:42,129:INFO:[LightGBM] [Info] Start training from score -4.009903
2024-07-01 19:42:42,129:INFO:[LightGBM] [Info] Start training from score -4.019382
2024-07-01 19:42:42,129:INFO:[LightGBM] [Info] Start training from score -5.009780
2024-07-01 19:42:42,130:INFO:[LightGBM] [Info] Start training from score -5.009780
2024-07-01 19:42:42,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:42,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:45,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:45,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:45,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:45,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:45,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,533:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,547:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,557:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,583:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:46,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,258:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,367:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,533:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,564:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,564:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:47,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 19:42:48,215:INFO:Uploading results into container
2024-07-01 19:42:48,217:INFO:Uploading model into container now
2024-07-01 19:42:48,247:INFO:_master_model_container: 7
2024-07-01 19:42:48,247:INFO:_display_container: 3
2024-07-01 19:42:48,248:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-07-01 19:42:48,248:INFO:create_model() successfully completed......................................
2024-07-01 19:42:48,425:INFO:Initializing evaluate_model()
2024-07-01 19:42:48,426:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-07-01 19:42:48,464:INFO:Initializing plot_model()
2024-07-01 19:42:48,465:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-07-01 19:42:48,465:INFO:Checking exceptions
2024-07-01 19:42:48,481:INFO:Preloading libraries
2024-07-01 19:42:48,999:INFO:Copying training dataset
2024-07-01 19:42:48,999:INFO:Plot type: pipeline
2024-07-01 19:42:49,361:INFO:Visual Rendered Successfully
2024-07-01 19:42:49,463:INFO:plot_model() successfully completed......................................
2024-07-01 19:42:49,499:INFO:Initializing predict_model()
2024-07-01 19:42:49,499:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002063B267F60>)
2024-07-01 19:42:49,500:INFO:Checking exceptions
2024-07-01 19:42:49,500:INFO:Preloading libraries
2024-07-01 19:42:50,483:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:50,491:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:50,498:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:50,630:INFO:Initializing predict_model()
2024-07-01 19:42:50,630:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000206333AE0C0>)
2024-07-01 19:42:50,631:INFO:Checking exceptions
2024-07-01 19:42:50,631:INFO:Preloading libraries
2024-07-01 19:42:50,638:INFO:Set up data.
2024-07-01 19:42:50,748:INFO:Set up index.
2024-07-01 19:42:52,839:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:52,848:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:52,855:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 19:42:52,993:INFO:Initializing plot_model()
2024-07-01 19:42:52,993:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-01 19:42:52,993:INFO:Checking exceptions
2024-07-01 19:42:53,010:INFO:Preloading libraries
2024-07-01 19:42:53,503:INFO:Copying training dataset
2024-07-01 19:42:53,503:INFO:Plot type: feature
2024-07-01 19:42:53,504:WARNING:No coef_ found. Trying feature_importances_
2024-07-01 19:42:53,796:INFO:Visual Rendered Successfully
2024-07-01 19:42:53,877:INFO:plot_model() successfully completed......................................
2024-07-01 19:58:57,022:INFO:Initializing plot_model()
2024-07-01 19:58:57,023:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=parameter, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-07-01 19:58:57,023:INFO:Checking exceptions
2024-07-01 19:58:57,036:INFO:Preloading libraries
2024-07-01 19:58:57,578:INFO:Copying training dataset
2024-07-01 19:58:57,579:INFO:Plot type: parameter
2024-07-01 19:58:57,586:INFO:Visual Rendered Successfully
2024-07-01 19:58:57,727:INFO:plot_model() successfully completed......................................
2024-07-01 19:58:58,680:INFO:Initializing plot_model()
2024-07-01 19:58:58,680:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=auc, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-07-01 19:58:58,680:INFO:Checking exceptions
2024-07-01 19:58:58,692:INFO:Preloading libraries
2024-07-01 19:58:59,092:INFO:Copying training dataset
2024-07-01 19:58:59,092:INFO:Plot type: auc
2024-07-01 19:58:59,319:INFO:Fitting Model
2024-07-01 19:58:59,320:INFO:Scoring test/hold-out set
2024-07-01 19:59:00,324:INFO:Visual Rendered Successfully
2024-07-01 19:59:00,401:INFO:plot_model() successfully completed......................................
2024-07-01 19:59:08,583:INFO:Initializing plot_model()
2024-07-01 19:59:08,583:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=learning, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-07-01 19:59:08,584:INFO:Checking exceptions
2024-07-01 19:59:08,598:INFO:Preloading libraries
2024-07-01 19:59:09,012:INFO:Copying training dataset
2024-07-01 19:59:09,012:INFO:Plot type: learning
2024-07-01 19:59:09,219:INFO:Fitting Model
2024-07-01 20:10:10,596:INFO:Initializing plot_model()
2024-07-01 20:10:10,596:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=boundary, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-07-01 20:10:10,597:INFO:Checking exceptions
2024-07-01 20:10:10,641:INFO:Preloading libraries
2024-07-01 20:10:11,158:INFO:Copying training dataset
2024-07-01 20:10:11,158:INFO:Plot type: boundary
2024-07-01 20:10:11,285:INFO:Fitting StandardScaler()
2024-07-01 20:10:11,300:INFO:Fitting PCA()
2024-07-01 20:12:12,978:INFO:Initializing plot_model()
2024-07-01 20:12:12,979:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-07-01 20:12:12,979:INFO:Checking exceptions
2024-07-01 20:12:12,991:INFO:Preloading libraries
2024-07-01 20:12:13,577:INFO:Copying training dataset
2024-07-01 20:12:13,577:INFO:Plot type: pipeline
2024-07-01 20:12:13,774:INFO:Visual Rendered Successfully
2024-07-01 20:12:13,938:INFO:plot_model() successfully completed......................................
2024-07-01 20:22:10,945:INFO:Initializing tune_model()
2024-07-01 20:22:10,945:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-07-01 20:22:10,945:INFO:Checking exceptions
2024-07-01 20:22:11,013:INFO:Copying training dataset
2024-07-01 20:22:11,078:INFO:Checking base model
2024-07-01 20:22:11,078:INFO:Base model : Light Gradient Boosting Machine
2024-07-01 20:22:11,114:INFO:Declaring metric variables
2024-07-01 20:22:11,124:INFO:Defining Hyperparameters
2024-07-01 20:22:11,383:INFO:Tuning with n_jobs=-1
2024-07-01 20:22:11,383:INFO:Initializing RandomizedSearchCV
2024-07-01 20:29:22,494:INFO:best_params: {'actual_estimator__reg_lambda': 0.7, 'actual_estimator__reg_alpha': 2, 'actual_estimator__num_leaves': 200, 'actual_estimator__n_estimators': 60, 'actual_estimator__min_split_gain': 0.1, 'actual_estimator__min_child_samples': 66, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.5, 'actual_estimator__bagging_freq': 5, 'actual_estimator__bagging_fraction': 0.7}
2024-07-01 20:29:22,502:INFO:Hyperparameter search completed
2024-07-01 20:29:22,502:INFO:SubProcess create_model() called ==================================
2024-07-01 20:29:22,502:INFO:Initializing create_model()
2024-07-01 20:29:22,502:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002064CCE96D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 0.7, 'reg_alpha': 2, 'num_leaves': 200, 'n_estimators': 60, 'min_split_gain': 0.1, 'min_child_samples': 66, 'learning_rate': 0.1, 'feature_fraction': 0.5, 'bagging_freq': 5, 'bagging_fraction': 0.7})
2024-07-01 20:29:22,502:INFO:Checking exceptions
2024-07-01 20:29:22,502:INFO:Importing libraries
2024-07-01 20:29:22,502:INFO:Copying training dataset
2024-07-01 20:29:22,566:INFO:Defining folds
2024-07-01 20:29:22,574:INFO:Declaring metric variables
2024-07-01 20:29:22,582:INFO:Importing untrained model
2024-07-01 20:29:22,582:INFO:Declaring custom model
2024-07-01 20:29:22,598:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-01 20:29:22,614:INFO:Starting cross validation
2024-07-01 20:29:22,622:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 20:29:27,927:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:27,937:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:27,949:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:28,079:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:28,089:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:28,099:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:29,428:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:29,435:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:29,447:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:29,613:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:29,623:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:29,630:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:33,463:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:33,473:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:33,509:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:33,760:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:33,768:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:33,776:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:34,951:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:34,959:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:34,999:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:35,343:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:35,351:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:35,359:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:38,313:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:38,321:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:38,337:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:38,505:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:38,513:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:38,529:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:29:38,553:INFO:Calculating mean and std
2024-07-01 20:29:38,553:INFO:Creating metrics dataframe
2024-07-01 20:29:38,577:INFO:Finalizing model
2024-07-01 20:29:38,633:INFO:[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5
2024-07-01 20:29:38,633:INFO:[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7
2024-07-01 20:29:38,633:INFO:[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5
2024-07-01 20:29:38,665:INFO:[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5
2024-07-01 20:29:38,665:INFO:[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7
2024-07-01 20:29:38,665:INFO:[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5
2024-07-01 20:29:38,673:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000716 seconds.
2024-07-01 20:29:38,673:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-01 20:29:38,673:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-01 20:29:38,673:INFO:[LightGBM] [Info] Total Bins 3492
2024-07-01 20:29:38,681:INFO:[LightGBM] [Info] Number of data points in the train set: 5845, number of used features: 34
2024-07-01 20:29:38,681:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 20:29:38,681:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 20:29:38,681:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:29:38,681:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:29:38,681:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -3.049324
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -3.063870
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -3.060214
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -4.009903
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -4.019382
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -5.009780
2024-07-01 20:29:38,689:INFO:[LightGBM] [Info] Start training from score -5.009780
2024-07-01 20:29:38,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:38,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,258:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,367:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,371:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,371:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,536:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,536:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,619:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,682:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:39,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,557:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,580:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,619:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,682:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:40,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,580:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,583:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,668:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:41,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:29:42,002:INFO:Uploading results into container
2024-07-01 20:29:42,010:INFO:Uploading model into container now
2024-07-01 20:29:42,010:INFO:_master_model_container: 8
2024-07-01 20:29:42,010:INFO:_display_container: 6
2024-07-01 20:29:42,010:INFO:LGBMClassifier(bagging_fraction=0.7, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=66, min_child_weight=0.001, min_split_gain=0.1,
               n_estimators=60, n_jobs=-1, num_leaves=200, objective=None,
               random_state=1663, reg_alpha=2, reg_lambda=0.7, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-07-01 20:29:42,010:INFO:create_model() successfully completed......................................
2024-07-01 20:29:42,434:INFO:SubProcess create_model() end ==================================
2024-07-01 20:29:42,434:INFO:choose_better activated
2024-07-01 20:29:42,450:INFO:SubProcess create_model() called ==================================
2024-07-01 20:29:42,450:INFO:Initializing create_model()
2024-07-01 20:29:42,450:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000206352D0E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-01 20:29:42,450:INFO:Checking exceptions
2024-07-01 20:29:42,458:INFO:Importing libraries
2024-07-01 20:29:42,458:INFO:Copying training dataset
2024-07-01 20:29:42,514:INFO:Defining folds
2024-07-01 20:29:42,514:INFO:Declaring metric variables
2024-07-01 20:29:42,514:INFO:Importing untrained model
2024-07-01 20:29:42,514:INFO:Declaring custom model
2024-07-01 20:29:42,514:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-01 20:29:42,522:INFO:Starting cross validation
2024-07-01 20:29:42,530:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-01 20:30:09,546:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:09,554:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:09,594:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:09,881:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:09,889:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:09,897:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:11,209:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:11,217:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:11,225:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:11,528:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:11,536:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:11,547:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:37,002:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:37,018:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:37,026:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:37,282:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:37,298:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:37,306:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:37,813:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:37,821:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:37,825:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:38,857:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:38,865:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:38,865:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:52,504:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:52,519:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:52,530:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:53,148:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:53,158:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:53,168:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 24) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-07-01 20:30:53,197:INFO:Calculating mean and std
2024-07-01 20:30:53,198:INFO:Creating metrics dataframe
2024-07-01 20:30:53,203:INFO:Finalizing model
2024-07-01 20:30:53,287:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003007 seconds.
2024-07-01 20:30:53,287:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-07-01 20:30:53,287:INFO:[LightGBM] [Info] Total Bins 3492
2024-07-01 20:30:53,287:INFO:[LightGBM] [Info] Number of data points in the train set: 5845, number of used features: 34
2024-07-01 20:30:53,289:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 20:30:53,289:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 20:30:53,289:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:30:53,289:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:30:53,289:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:30:53,289:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 20:30:53,289:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 20:30:53,289:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 20:30:53,289:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:30:53,293:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:30:53,293:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:30:53,293:INFO:[LightGBM] [Info] Start training from score -3.042130
2024-07-01 20:30:53,293:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:30:53,293:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:30:53,293:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:30:53,293:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:30:53,293:INFO:[LightGBM] [Info] Start training from score -3.049324
2024-07-01 20:30:53,293:INFO:[LightGBM] [Info] Start training from score -3.045721
2024-07-01 20:30:53,293:INFO:[LightGBM] [Info] Start training from score -3.063870
2024-07-01 20:30:53,293:INFO:[LightGBM] [Info] Start training from score -3.060214
2024-07-01 20:30:53,293:INFO:[LightGBM] [Info] Start training from score -4.009903
2024-07-01 20:30:53,293:INFO:[LightGBM] [Info] Start training from score -4.019382
2024-07-01 20:30:53,293:INFO:[LightGBM] [Info] Start training from score -5.009780
2024-07-01 20:30:53,293:INFO:[LightGBM] [Info] Start training from score -5.009780
2024-07-01 20:30:53,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:53,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:59,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:59,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:30:59,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,515:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:00,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:01,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:02,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-01 20:31:03,708:INFO:Uploading results into container
2024-07-01 20:31:03,708:INFO:Uploading model into container now
2024-07-01 20:31:03,708:INFO:_master_model_container: 9
2024-07-01 20:31:03,708:INFO:_display_container: 7
2024-07-01 20:31:03,712:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-07-01 20:31:03,712:INFO:create_model() successfully completed......................................
2024-07-01 20:31:03,873:INFO:SubProcess create_model() end ==================================
2024-07-01 20:31:03,873:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1663, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8616
2024-07-01 20:31:03,873:INFO:LGBMClassifier(bagging_fraction=0.7, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=66, min_child_weight=0.001, min_split_gain=0.1,
               n_estimators=60, n_jobs=-1, num_leaves=200, objective=None,
               random_state=1663, reg_alpha=2, reg_lambda=0.7, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8626
2024-07-01 20:31:03,873:INFO:LGBMClassifier(bagging_fraction=0.7, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=66, min_child_weight=0.001, min_split_gain=0.1,
               n_estimators=60, n_jobs=-1, num_leaves=200, objective=None,
               random_state=1663, reg_alpha=2, reg_lambda=0.7, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-07-01 20:31:03,873:INFO:choose_better completed
2024-07-01 20:31:03,898:INFO:_master_model_container: 9
2024-07-01 20:31:03,898:INFO:_display_container: 6
2024-07-01 20:31:03,898:INFO:LGBMClassifier(bagging_fraction=0.7, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=66, min_child_weight=0.001, min_split_gain=0.1,
               n_estimators=60, n_jobs=-1, num_leaves=200, objective=None,
               random_state=1663, reg_alpha=2, reg_lambda=0.7, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-07-01 20:31:03,898:INFO:tune_model() successfully completed......................................
2024-07-02 12:10:10,375:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-02 12:10:10,375:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-02 12:10:10,376:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-02 12:10:10,376:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-02 12:10:10,791:INFO:PyCaret RegressionExperiment
2024-07-02 12:10:10,792:INFO:Logging name: reg-default-name
2024-07-02 12:10:10,796:INFO:ML Usecase: MLUsecase.REGRESSION
2024-07-02 12:10:10,796:INFO:version 3.3.2
2024-07-02 12:10:10,797:INFO:Initializing setup()
2024-07-02 12:10:10,797:INFO:self.USI: 0250
2024-07-02 12:10:10,797:INFO:self._variable_keys: {'X_train', 'y', 'fold_generator', 'memory', 'html_param', 'idx', 'X', 'log_plots_param', 'USI', 'gpu_param', 'gpu_n_jobs_param', 'seed', 'pipeline', 'target_param', '_available_plots', 'y_train', 'logging_param', '_ml_usecase', 'exp_name_log', 'transform_target_param', 'exp_id', 'fold_groups_param', 'X_test', 'data', 'y_test', 'fold_shuffle_param', 'n_jobs_param'}
2024-07-02 12:10:10,797:INFO:Checking environment
2024-07-02 12:10:10,797:INFO:python_version: 3.11.9
2024-07-02 12:10:10,798:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-07-02 12:10:10,799:INFO:machine: AMD64
2024-07-02 12:10:10,799:INFO:platform: Windows-10-10.0.19045-SP0
2024-07-02 12:10:10,807:INFO:Memory: svmem(total=17049022464, available=7641513984, percent=55.2, used=9407508480, free=7641513984)
2024-07-02 12:10:10,807:INFO:Physical Core: 2
2024-07-02 12:10:10,807:INFO:Logical Core: 4
2024-07-02 12:10:10,808:INFO:Checking libraries
2024-07-02 12:10:10,808:INFO:System:
2024-07-02 12:10:10,808:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-07-02 12:10:10,808:INFO:executable: e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Scripts\python.exe
2024-07-02 12:10:10,808:INFO:   machine: Windows-10-10.0.19045-SP0
2024-07-02 12:10:10,808:INFO:PyCaret required dependencies:
2024-07-02 12:10:11,116:INFO:                 pip: 24.0
2024-07-02 12:10:11,116:INFO:          setuptools: 70.1.1
2024-07-02 12:10:11,116:INFO:             pycaret: 3.3.2
2024-07-02 12:10:11,116:INFO:             IPython: 8.26.0
2024-07-02 12:10:11,116:INFO:          ipywidgets: 8.1.3
2024-07-02 12:10:11,117:INFO:                tqdm: 4.66.4
2024-07-02 12:10:11,117:INFO:               numpy: 1.26.4
2024-07-02 12:10:11,117:INFO:              pandas: 2.1.4
2024-07-02 12:10:11,117:INFO:              jinja2: 3.1.4
2024-07-02 12:10:11,117:INFO:               scipy: 1.11.4
2024-07-02 12:10:11,118:INFO:              joblib: 1.3.2
2024-07-02 12:10:11,118:INFO:             sklearn: 1.4.2
2024-07-02 12:10:11,118:INFO:                pyod: 2.0.1
2024-07-02 12:10:11,118:INFO:            imblearn: 0.12.3
2024-07-02 12:10:11,118:INFO:   category_encoders: 2.6.3
2024-07-02 12:10:11,118:INFO:            lightgbm: 4.4.0
2024-07-02 12:10:11,119:INFO:               numba: 0.60.0
2024-07-02 12:10:11,119:INFO:            requests: 2.32.3
2024-07-02 12:10:11,119:INFO:          matplotlib: 3.7.5
2024-07-02 12:10:11,119:INFO:          scikitplot: 0.3.7
2024-07-02 12:10:11,119:INFO:         yellowbrick: 1.5
2024-07-02 12:10:11,119:INFO:              plotly: 5.22.0
2024-07-02 12:10:11,119:INFO:    plotly-resampler: Not installed
2024-07-02 12:10:11,119:INFO:             kaleido: 0.2.1
2024-07-02 12:10:11,119:INFO:           schemdraw: 0.15
2024-07-02 12:10:11,120:INFO:         statsmodels: 0.14.2
2024-07-02 12:10:11,120:INFO:              sktime: 0.26.0
2024-07-02 12:10:11,120:INFO:               tbats: 1.1.3
2024-07-02 12:10:11,120:INFO:            pmdarima: 2.0.4
2024-07-02 12:10:11,120:INFO:              psutil: 6.0.0
2024-07-02 12:10:11,120:INFO:          markupsafe: 2.1.5
2024-07-02 12:10:11,120:INFO:             pickle5: Not installed
2024-07-02 12:10:11,120:INFO:         cloudpickle: 3.0.0
2024-07-02 12:10:11,120:INFO:         deprecation: 2.1.0
2024-07-02 12:10:11,120:INFO:              xxhash: 3.4.1
2024-07-02 12:10:11,120:INFO:           wurlitzer: Not installed
2024-07-02 12:10:11,121:INFO:PyCaret optional dependencies:
2024-07-02 12:10:11,162:INFO:                shap: 0.46.0
2024-07-02 12:10:11,162:INFO:           interpret: Not installed
2024-07-02 12:10:11,162:INFO:                umap: Not installed
2024-07-02 12:10:11,162:INFO:     ydata_profiling: Not installed
2024-07-02 12:10:11,163:INFO:  explainerdashboard: Not installed
2024-07-02 12:10:11,163:INFO:             autoviz: Not installed
2024-07-02 12:10:11,163:INFO:           fairlearn: Not installed
2024-07-02 12:10:11,163:INFO:          deepchecks: Not installed
2024-07-02 12:10:11,163:INFO:             xgboost: 2.1.0
2024-07-02 12:10:11,163:INFO:            catboost: 1.2.5
2024-07-02 12:10:11,163:INFO:              kmodes: Not installed
2024-07-02 12:10:11,163:INFO:             mlxtend: Not installed
2024-07-02 12:10:11,163:INFO:       statsforecast: Not installed
2024-07-02 12:10:11,163:INFO:        tune_sklearn: Not installed
2024-07-02 12:10:11,163:INFO:                 ray: Not installed
2024-07-02 12:10:11,163:INFO:            hyperopt: Not installed
2024-07-02 12:10:11,163:INFO:              optuna: Not installed
2024-07-02 12:10:11,164:INFO:               skopt: Not installed
2024-07-02 12:10:11,164:INFO:              mlflow: Not installed
2024-07-02 12:10:11,164:INFO:              gradio: Not installed
2024-07-02 12:10:11,164:INFO:             fastapi: Not installed
2024-07-02 12:10:11,164:INFO:             uvicorn: Not installed
2024-07-02 12:10:11,164:INFO:              m2cgen: Not installed
2024-07-02 12:10:11,164:INFO:           evidently: Not installed
2024-07-02 12:10:11,164:INFO:               fugue: Not installed
2024-07-02 12:10:11,164:INFO:           streamlit: Not installed
2024-07-02 12:10:11,164:INFO:             prophet: Not installed
2024-07-02 12:10:11,164:INFO:None
2024-07-02 12:10:11,164:INFO:Set up data.
2024-07-02 12:10:11,479:INFO:Set up folding strategy.
2024-07-02 12:10:11,480:INFO:Set up train/test split.
2024-07-02 12:10:11,689:INFO:Set up index.
2024-07-02 12:10:11,690:INFO:Assigning column types.
2024-07-02 12:10:11,720:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-07-02 12:10:11,720:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-02 12:10:11,746:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-02 12:10:11,779:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-02 12:10:12,012:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-02 12:10:12,246:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-02 12:10:12,248:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-02 12:10:12,252:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-02 12:10:13,484:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-02 12:10:13,503:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-02 12:10:13,514:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-02 12:10:13,746:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-02 12:10:13,892:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-02 12:10:13,896:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-02 12:10:13,902:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-02 12:10:13,903:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-07-02 12:10:13,919:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-02 12:10:13,932:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-02 12:10:14,145:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-02 12:10:14,271:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-02 12:10:14,274:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-02 12:10:14,281:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-02 12:10:14,315:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-02 12:10:14,323:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-02 12:10:14,526:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-02 12:10:14,658:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-02 12:10:14,659:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-02 12:10:14,662:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-02 12:10:14,663:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-07-02 12:10:14,697:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-02 12:10:14,932:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-02 12:10:15,082:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-02 12:10:15,083:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-02 12:10:15,101:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-02 12:10:15,127:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-02 12:10:15,341:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-02 12:10:15,528:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-02 12:10:15,529:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-02 12:10:15,543:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-02 12:10:15,547:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-07-02 12:10:15,840:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-02 12:10:16,011:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-02 12:10:16,011:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-02 12:10:16,027:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-02 12:10:16,266:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-02 12:10:16,397:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-02 12:10:16,400:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-02 12:10:16,409:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-02 12:10:16,414:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-07-02 12:10:16,657:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-02 12:10:16,787:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-02 12:10:16,793:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-02 12:10:17,028:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-02 12:10:17,233:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-02 12:10:17,244:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-02 12:10:17,245:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-07-02 12:10:17,810:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-02 12:10:17,837:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-02 12:10:18,310:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-02 12:10:18,322:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-02 12:10:18,361:INFO:Preparing preprocessing pipeline...
2024-07-02 12:10:18,361:INFO:Set up simple imputation.
2024-07-02 12:10:18,538:INFO:Finished creating preprocessing pipeline.
2024-07-02 12:10:18,560:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\HP\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'experience',
                                             'results_grid', 'quali_position',
                                             'average_lap_time',
                                             'average_finish',
                                             'driver_standings_points',
                                             'driver_standings_position',
                                             'const_standing_points',
                                             'const_standing_position'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent')))])
2024-07-02 12:10:18,561:INFO:Creating final display dataframe.
2024-07-02 12:10:19,127:INFO:Setup _display_container:                     Description                  Value
0                    Session id                   2570
1                        Target  results_positionOrder
2                   Target type             Regression
3           Original data shape             (8350, 66)
4        Transformed data shape             (8350, 35)
5   Transformed train set shape             (5845, 35)
6    Transformed test set shape             (2505, 35)
7               Ignore features                     31
8              Numeric features                     10
9      Rows with missing values                 100.0%
10                   Preprocess                   True
11              Imputation type                 simple
12           Numeric imputation                   mean
13       Categorical imputation                   mode
14               Fold Generator                  KFold
15                  Fold Number                     10
16                     CPU Jobs                     -1
17                      Use GPU                  False
18               Log Experiment                  False
19              Experiment Name       reg-default-name
20                          USI                   0250
2024-07-02 12:10:19,668:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-02 12:10:19,675:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-02 12:10:20,067:INFO:Soft dependency imported: xgboost: 2.1.0
2024-07-02 12:10:20,074:INFO:Soft dependency imported: catboost: 1.2.5
2024-07-02 12:10:20,075:INFO:setup() successfully completed in 9.29s...............
2024-07-02 12:10:48,756:INFO:Initializing compare_models()
2024-07-02 12:10:48,756:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>})
2024-07-02 12:10:48,756:INFO:Checking exceptions
2024-07-02 12:10:48,773:INFO:Preparing display monitor
2024-07-02 12:10:48,867:INFO:Initializing Linear Regression
2024-07-02 12:10:48,867:INFO:Total runtime is 0.0 minutes
2024-07-02 12:10:48,905:INFO:SubProcess create_model() called ==================================
2024-07-02 12:10:48,927:INFO:Initializing create_model()
2024-07-02 12:10:48,927:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:10:48,927:INFO:Checking exceptions
2024-07-02 12:10:48,927:INFO:Importing libraries
2024-07-02 12:10:48,927:INFO:Copying training dataset
2024-07-02 12:10:49,126:INFO:Defining folds
2024-07-02 12:10:49,127:INFO:Declaring metric variables
2024-07-02 12:10:49,138:INFO:Importing untrained model
2024-07-02 12:10:49,145:INFO:Linear Regression Imported successfully
2024-07-02 12:10:49,230:INFO:Starting cross validation
2024-07-02 12:10:49,251:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:05,207:WARNING:create_model() for lr raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-02 12:11:05,245:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_base.py", line 578, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
LinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:05,245:INFO:Initializing create_model()
2024-07-02 12:11:05,245:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:05,251:INFO:Checking exceptions
2024-07-02 12:11:05,252:INFO:Importing libraries
2024-07-02 12:11:05,252:INFO:Copying training dataset
2024-07-02 12:11:05,307:INFO:Defining folds
2024-07-02 12:11:05,307:INFO:Declaring metric variables
2024-07-02 12:11:05,322:INFO:Importing untrained model
2024-07-02 12:11:05,336:INFO:Linear Regression Imported successfully
2024-07-02 12:11:05,356:INFO:Starting cross validation
2024-07-02 12:11:05,358:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:05,662:ERROR:create_model() for lr raised an exception or returned all 0.0:
2024-07-02 12:11:05,668:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_base.py", line 578, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
LinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_base.py", line 578, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
LinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:05,669:INFO:Initializing Lasso Regression
2024-07-02 12:11:05,669:INFO:Total runtime is 0.2800293922424316 minutes
2024-07-02 12:11:05,672:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:05,673:INFO:Initializing create_model()
2024-07-02 12:11:05,673:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:05,673:INFO:Checking exceptions
2024-07-02 12:11:05,674:INFO:Importing libraries
2024-07-02 12:11:05,674:INFO:Copying training dataset
2024-07-02 12:11:05,772:INFO:Defining folds
2024-07-02 12:11:05,773:INFO:Declaring metric variables
2024-07-02 12:11:05,785:INFO:Importing untrained model
2024-07-02 12:11:05,826:INFO:Lasso Regression Imported successfully
2024-07-02 12:11:05,844:INFO:Starting cross validation
2024-07-02 12:11:05,848:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:06,149:WARNING:create_model() for lasso raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-02 12:11:06,152:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 955, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:06,153:INFO:Initializing create_model()
2024-07-02 12:11:06,153:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:06,153:INFO:Checking exceptions
2024-07-02 12:11:06,153:INFO:Importing libraries
2024-07-02 12:11:06,153:INFO:Copying training dataset
2024-07-02 12:11:06,212:INFO:Defining folds
2024-07-02 12:11:06,216:INFO:Declaring metric variables
2024-07-02 12:11:06,227:INFO:Importing untrained model
2024-07-02 12:11:06,243:INFO:Lasso Regression Imported successfully
2024-07-02 12:11:06,270:INFO:Starting cross validation
2024-07-02 12:11:06,274:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:06,531:ERROR:create_model() for lasso raised an exception or returned all 0.0:
2024-07-02 12:11:06,537:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 955, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 955, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:06,538:INFO:Initializing Ridge Regression
2024-07-02 12:11:06,538:INFO:Total runtime is 0.2945117394129435 minutes
2024-07-02 12:11:06,547:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:06,548:INFO:Initializing create_model()
2024-07-02 12:11:06,548:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:06,549:INFO:Checking exceptions
2024-07-02 12:11:06,549:INFO:Importing libraries
2024-07-02 12:11:06,550:INFO:Copying training dataset
2024-07-02 12:11:06,592:INFO:Defining folds
2024-07-02 12:11:06,592:INFO:Declaring metric variables
2024-07-02 12:11:06,599:INFO:Importing untrained model
2024-07-02 12:11:06,610:INFO:Ridge Regression Imported successfully
2024-07-02 12:11:06,625:INFO:Starting cross validation
2024-07-02 12:11:06,628:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:06,942:WARNING:create_model() for ridge raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-02 12:11:06,944:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_ridge.py", line 1167, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
Ridge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:06,946:INFO:Initializing create_model()
2024-07-02 12:11:06,946:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:06,946:INFO:Checking exceptions
2024-07-02 12:11:06,946:INFO:Importing libraries
2024-07-02 12:11:06,947:INFO:Copying training dataset
2024-07-02 12:11:06,976:INFO:Defining folds
2024-07-02 12:11:06,977:INFO:Declaring metric variables
2024-07-02 12:11:06,991:INFO:Importing untrained model
2024-07-02 12:11:07,006:INFO:Ridge Regression Imported successfully
2024-07-02 12:11:07,021:INFO:Starting cross validation
2024-07-02 12:11:07,025:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:07,455:ERROR:create_model() for ridge raised an exception or returned all 0.0:
2024-07-02 12:11:07,483:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_ridge.py", line 1167, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
Ridge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_ridge.py", line 1167, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
Ridge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:07,484:INFO:Initializing Elastic Net
2024-07-02 12:11:07,485:INFO:Total runtime is 0.31029013395309446 minutes
2024-07-02 12:11:07,503:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:07,503:INFO:Initializing create_model()
2024-07-02 12:11:07,504:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:07,504:INFO:Checking exceptions
2024-07-02 12:11:07,504:INFO:Importing libraries
2024-07-02 12:11:07,504:INFO:Copying training dataset
2024-07-02 12:11:07,562:INFO:Defining folds
2024-07-02 12:11:07,564:INFO:Declaring metric variables
2024-07-02 12:11:07,571:INFO:Importing untrained model
2024-07-02 12:11:07,584:INFO:Elastic Net Imported successfully
2024-07-02 12:11:07,621:INFO:Starting cross validation
2024-07-02 12:11:07,625:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:07,925:WARNING:create_model() for en raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-02 12:11:07,929:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 955, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
ElasticNet does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:07,932:INFO:Initializing create_model()
2024-07-02 12:11:07,933:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:07,933:INFO:Checking exceptions
2024-07-02 12:11:07,933:INFO:Importing libraries
2024-07-02 12:11:07,933:INFO:Copying training dataset
2024-07-02 12:11:07,980:INFO:Defining folds
2024-07-02 12:11:07,981:INFO:Declaring metric variables
2024-07-02 12:11:07,998:INFO:Importing untrained model
2024-07-02 12:11:08,008:INFO:Elastic Net Imported successfully
2024-07-02 12:11:08,042:INFO:Starting cross validation
2024-07-02 12:11:08,048:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:08,378:ERROR:create_model() for en raised an exception or returned all 0.0:
2024-07-02 12:11:08,385:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 955, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
ElasticNet does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 955, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
ElasticNet does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:08,385:INFO:Initializing Least Angle Regression
2024-07-02 12:11:08,386:INFO:Total runtime is 0.3253177960713704 minutes
2024-07-02 12:11:08,390:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:08,391:INFO:Initializing create_model()
2024-07-02 12:11:08,391:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:08,391:INFO:Checking exceptions
2024-07-02 12:11:08,391:INFO:Importing libraries
2024-07-02 12:11:08,391:INFO:Copying training dataset
2024-07-02 12:11:08,466:INFO:Defining folds
2024-07-02 12:11:08,466:INFO:Declaring metric variables
2024-07-02 12:11:08,473:INFO:Importing untrained model
2024-07-02 12:11:08,490:INFO:Least Angle Regression Imported successfully
2024-07-02 12:11:08,524:INFO:Starting cross validation
2024-07-02 12:11:08,533:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:08,878:WARNING:create_model() for lar raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-02 12:11:08,882:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_least_angle.py", line 1146, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
Lars does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:08,883:INFO:Initializing create_model()
2024-07-02 12:11:08,883:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:08,883:INFO:Checking exceptions
2024-07-02 12:11:08,883:INFO:Importing libraries
2024-07-02 12:11:08,883:INFO:Copying training dataset
2024-07-02 12:11:08,917:INFO:Defining folds
2024-07-02 12:11:08,917:INFO:Declaring metric variables
2024-07-02 12:11:08,930:INFO:Importing untrained model
2024-07-02 12:11:08,948:INFO:Least Angle Regression Imported successfully
2024-07-02 12:11:08,984:INFO:Starting cross validation
2024-07-02 12:11:08,987:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:09,282:ERROR:create_model() for lar raised an exception or returned all 0.0:
2024-07-02 12:11:09,284:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_least_angle.py", line 1146, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
Lars does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_least_angle.py", line 1146, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
Lars does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:09,284:INFO:Initializing Lasso Least Angle Regression
2024-07-02 12:11:09,285:INFO:Total runtime is 0.34029579559961953 minutes
2024-07-02 12:11:09,288:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:09,289:INFO:Initializing create_model()
2024-07-02 12:11:09,289:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:09,289:INFO:Checking exceptions
2024-07-02 12:11:09,289:INFO:Importing libraries
2024-07-02 12:11:09,289:INFO:Copying training dataset
2024-07-02 12:11:09,342:INFO:Defining folds
2024-07-02 12:11:09,350:INFO:Declaring metric variables
2024-07-02 12:11:09,358:INFO:Importing untrained model
2024-07-02 12:11:09,370:INFO:Lasso Least Angle Regression Imported successfully
2024-07-02 12:11:09,399:INFO:Starting cross validation
2024-07-02 12:11:09,402:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:09,699:WARNING:create_model() for llar raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-02 12:11:09,700:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_least_angle.py", line 1146, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
LassoLars does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:09,701:INFO:Initializing create_model()
2024-07-02 12:11:09,701:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:09,701:INFO:Checking exceptions
2024-07-02 12:11:09,701:INFO:Importing libraries
2024-07-02 12:11:09,701:INFO:Copying training dataset
2024-07-02 12:11:09,750:INFO:Defining folds
2024-07-02 12:11:09,750:INFO:Declaring metric variables
2024-07-02 12:11:09,768:INFO:Importing untrained model
2024-07-02 12:11:09,780:INFO:Lasso Least Angle Regression Imported successfully
2024-07-02 12:11:09,810:INFO:Starting cross validation
2024-07-02 12:11:09,813:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:10,088:ERROR:create_model() for llar raised an exception or returned all 0.0:
2024-07-02 12:11:10,097:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_least_angle.py", line 1146, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
LassoLars does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_least_angle.py", line 1146, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
LassoLars does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:10,098:INFO:Initializing Orthogonal Matching Pursuit
2024-07-02 12:11:10,098:INFO:Total runtime is 0.35384949445724484 minutes
2024-07-02 12:11:10,106:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:10,106:INFO:Initializing create_model()
2024-07-02 12:11:10,107:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:10,107:INFO:Checking exceptions
2024-07-02 12:11:10,107:INFO:Importing libraries
2024-07-02 12:11:10,107:INFO:Copying training dataset
2024-07-02 12:11:10,150:INFO:Defining folds
2024-07-02 12:11:10,150:INFO:Declaring metric variables
2024-07-02 12:11:10,156:INFO:Importing untrained model
2024-07-02 12:11:10,171:INFO:Orthogonal Matching Pursuit Imported successfully
2024-07-02 12:11:10,198:INFO:Starting cross validation
2024-07-02 12:11:10,200:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:10,503:WARNING:create_model() for omp raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-02 12:11:10,506:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_omp.py", line 750, in fit
    X, y = self._validate_data(X, y, multi_output=True, y_numeric=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
OrthogonalMatchingPursuit does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:10,506:INFO:Initializing create_model()
2024-07-02 12:11:10,506:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:10,506:INFO:Checking exceptions
2024-07-02 12:11:10,507:INFO:Importing libraries
2024-07-02 12:11:10,507:INFO:Copying training dataset
2024-07-02 12:11:10,571:INFO:Defining folds
2024-07-02 12:11:10,571:INFO:Declaring metric variables
2024-07-02 12:11:10,586:INFO:Importing untrained model
2024-07-02 12:11:10,596:INFO:Orthogonal Matching Pursuit Imported successfully
2024-07-02 12:11:10,619:INFO:Starting cross validation
2024-07-02 12:11:10,627:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:10,933:ERROR:create_model() for omp raised an exception or returned all 0.0:
2024-07-02 12:11:10,939:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_omp.py", line 750, in fit
    X, y = self._validate_data(X, y, multi_output=True, y_numeric=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
OrthogonalMatchingPursuit does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_omp.py", line 750, in fit
    X, y = self._validate_data(X, y, multi_output=True, y_numeric=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
OrthogonalMatchingPursuit does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:10,942:INFO:Initializing Bayesian Ridge
2024-07-02 12:11:10,943:INFO:Total runtime is 0.36792666912078853 minutes
2024-07-02 12:11:10,955:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:10,955:INFO:Initializing create_model()
2024-07-02 12:11:10,956:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:10,956:INFO:Checking exceptions
2024-07-02 12:11:10,956:INFO:Importing libraries
2024-07-02 12:11:10,956:INFO:Copying training dataset
2024-07-02 12:11:11,013:INFO:Defining folds
2024-07-02 12:11:11,027:INFO:Declaring metric variables
2024-07-02 12:11:11,046:INFO:Importing untrained model
2024-07-02 12:11:11,052:INFO:Bayesian Ridge Imported successfully
2024-07-02 12:11:11,078:INFO:Starting cross validation
2024-07-02 12:11:11,080:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:11,419:WARNING:create_model() for br raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-02 12:11:11,424:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_bayes.py", line 296, in fit
    X, y = self._validate_data(X, y, dtype=[np.float64, np.float32], y_numeric=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
BayesianRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:11,428:INFO:Initializing create_model()
2024-07-02 12:11:11,428:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:11,428:INFO:Checking exceptions
2024-07-02 12:11:11,429:INFO:Importing libraries
2024-07-02 12:11:11,429:INFO:Copying training dataset
2024-07-02 12:11:11,520:INFO:Defining folds
2024-07-02 12:11:11,521:INFO:Declaring metric variables
2024-07-02 12:11:11,549:INFO:Importing untrained model
2024-07-02 12:11:11,555:INFO:Bayesian Ridge Imported successfully
2024-07-02 12:11:11,595:INFO:Starting cross validation
2024-07-02 12:11:11,600:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:11,931:ERROR:create_model() for br raised an exception or returned all 0.0:
2024-07-02 12:11:11,938:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_bayes.py", line 296, in fit
    X, y = self._validate_data(X, y, dtype=[np.float64, np.float32], y_numeric=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
BayesianRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_bayes.py", line 296, in fit
    X, y = self._validate_data(X, y, dtype=[np.float64, np.float32], y_numeric=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
BayesianRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:11,940:INFO:Initializing Passive Aggressive Regressor
2024-07-02 12:11:11,940:INFO:Total runtime is 0.38454943100611366 minutes
2024-07-02 12:11:11,949:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:11,949:INFO:Initializing create_model()
2024-07-02 12:11:11,949:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:11,950:INFO:Checking exceptions
2024-07-02 12:11:11,951:INFO:Importing libraries
2024-07-02 12:11:11,951:INFO:Copying training dataset
2024-07-02 12:11:11,981:INFO:Defining folds
2024-07-02 12:11:11,983:INFO:Declaring metric variables
2024-07-02 12:11:12,003:INFO:Importing untrained model
2024-07-02 12:11:12,019:INFO:Passive Aggressive Regressor Imported successfully
2024-07-02 12:11:12,059:INFO:Starting cross validation
2024-07-02 12:11:12,069:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:12,629:WARNING:create_model() for par raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-02 12:11:12,632:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_passive_aggressive.py", line 566, in fit
    return self._fit(
           ^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1557, in _fit
    self._partial_fit(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1456, in _partial_fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
PassiveAggressiveRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:12,632:INFO:Initializing create_model()
2024-07-02 12:11:12,632:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:12,632:INFO:Checking exceptions
2024-07-02 12:11:12,633:INFO:Importing libraries
2024-07-02 12:11:12,633:INFO:Copying training dataset
2024-07-02 12:11:12,703:INFO:Defining folds
2024-07-02 12:11:12,706:INFO:Declaring metric variables
2024-07-02 12:11:12,718:INFO:Importing untrained model
2024-07-02 12:11:12,731:INFO:Passive Aggressive Regressor Imported successfully
2024-07-02 12:11:12,766:INFO:Starting cross validation
2024-07-02 12:11:12,784:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:13,158:ERROR:create_model() for par raised an exception or returned all 0.0:
2024-07-02 12:11:13,161:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_passive_aggressive.py", line 566, in fit
    return self._fit(
           ^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1557, in _fit
    self._partial_fit(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1456, in _partial_fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
PassiveAggressiveRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_passive_aggressive.py", line 566, in fit
    return self._fit(
           ^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1557, in _fit
    self._partial_fit(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1456, in _partial_fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
PassiveAggressiveRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:13,161:INFO:Initializing Huber Regressor
2024-07-02 12:11:13,162:INFO:Total runtime is 0.40490701595942175 minutes
2024-07-02 12:11:13,170:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:13,171:INFO:Initializing create_model()
2024-07-02 12:11:13,175:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:13,176:INFO:Checking exceptions
2024-07-02 12:11:13,176:INFO:Importing libraries
2024-07-02 12:11:13,177:INFO:Copying training dataset
2024-07-02 12:11:13,219:INFO:Defining folds
2024-07-02 12:11:13,219:INFO:Declaring metric variables
2024-07-02 12:11:13,234:INFO:Importing untrained model
2024-07-02 12:11:13,259:INFO:Huber Regressor Imported successfully
2024-07-02 12:11:13,298:INFO:Starting cross validation
2024-07-02 12:11:13,301:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:13,618:WARNING:create_model() for huber raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-02 12:11:13,624:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_huber.py", line 297, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
HuberRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:13,626:INFO:Initializing create_model()
2024-07-02 12:11:13,628:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:13,630:INFO:Checking exceptions
2024-07-02 12:11:13,630:INFO:Importing libraries
2024-07-02 12:11:13,630:INFO:Copying training dataset
2024-07-02 12:11:13,678:INFO:Defining folds
2024-07-02 12:11:13,679:INFO:Declaring metric variables
2024-07-02 12:11:13,686:INFO:Importing untrained model
2024-07-02 12:11:13,694:INFO:Huber Regressor Imported successfully
2024-07-02 12:11:13,734:INFO:Starting cross validation
2024-07-02 12:11:13,737:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:14,000:ERROR:create_model() for huber raised an exception or returned all 0.0:
2024-07-02 12:11:14,003:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_huber.py", line 297, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
HuberRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\linear_model\_huber.py", line 297, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
HuberRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:14,004:INFO:Initializing K Neighbors Regressor
2024-07-02 12:11:14,005:INFO:Total runtime is 0.4189630190531412 minutes
2024-07-02 12:11:14,011:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:14,012:INFO:Initializing create_model()
2024-07-02 12:11:14,012:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:14,012:INFO:Checking exceptions
2024-07-02 12:11:14,012:INFO:Importing libraries
2024-07-02 12:11:14,012:INFO:Copying training dataset
2024-07-02 12:11:14,045:INFO:Defining folds
2024-07-02 12:11:14,045:INFO:Declaring metric variables
2024-07-02 12:11:14,050:INFO:Importing untrained model
2024-07-02 12:11:14,066:INFO:K Neighbors Regressor Imported successfully
2024-07-02 12:11:14,101:INFO:Starting cross validation
2024-07-02 12:11:14,103:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:14,424:WARNING:create_model() for knn raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-02 12:11:14,429:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\neighbors\_regression.py", line 223, in fit
    return self._fit(X, y)
           ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\neighbors\_base.py", line 476, in _fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
KNeighborsRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:14,430:INFO:Initializing create_model()
2024-07-02 12:11:14,430:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:14,430:INFO:Checking exceptions
2024-07-02 12:11:14,430:INFO:Importing libraries
2024-07-02 12:11:14,430:INFO:Copying training dataset
2024-07-02 12:11:14,458:INFO:Defining folds
2024-07-02 12:11:14,459:INFO:Declaring metric variables
2024-07-02 12:11:14,469:INFO:Importing untrained model
2024-07-02 12:11:14,491:INFO:K Neighbors Regressor Imported successfully
2024-07-02 12:11:14,516:INFO:Starting cross validation
2024-07-02 12:11:14,518:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:14,787:ERROR:create_model() for knn raised an exception or returned all 0.0:
2024-07-02 12:11:14,793:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\neighbors\_regression.py", line 223, in fit
    return self._fit(X, y)
           ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\neighbors\_base.py", line 476, in _fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
KNeighborsRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\neighbors\_regression.py", line 223, in fit
    return self._fit(X, y)
           ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\neighbors\_base.py", line 476, in _fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
KNeighborsRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:14,794:INFO:Initializing Decision Tree Regressor
2024-07-02 12:11:14,794:INFO:Total runtime is 0.4321094433466593 minutes
2024-07-02 12:11:14,801:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:14,801:INFO:Initializing create_model()
2024-07-02 12:11:14,802:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:14,802:INFO:Checking exceptions
2024-07-02 12:11:14,805:INFO:Importing libraries
2024-07-02 12:11:14,805:INFO:Copying training dataset
2024-07-02 12:11:14,855:INFO:Defining folds
2024-07-02 12:11:14,859:INFO:Declaring metric variables
2024-07-02 12:11:14,868:INFO:Importing untrained model
2024-07-02 12:11:14,892:INFO:Decision Tree Regressor Imported successfully
2024-07-02 12:11:14,917:INFO:Starting cross validation
2024-07-02 12:11:14,922:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:15,741:INFO:Calculating mean and std
2024-07-02 12:11:15,746:INFO:Creating metrics dataframe
2024-07-02 12:11:15,758:INFO:Uploading results into container
2024-07-02 12:11:15,759:INFO:Uploading model into container now
2024-07-02 12:11:15,761:INFO:_master_model_container: 1
2024-07-02 12:11:15,762:INFO:_display_container: 2
2024-07-02 12:11:15,764:INFO:DecisionTreeRegressor(random_state=2570)
2024-07-02 12:11:15,764:INFO:create_model() successfully completed......................................
2024-07-02 12:11:15,928:INFO:SubProcess create_model() end ==================================
2024-07-02 12:11:15,928:INFO:Creating metrics dataframe
2024-07-02 12:11:15,944:INFO:Initializing Random Forest Regressor
2024-07-02 12:11:15,945:INFO:Total runtime is 0.4513009667396545 minutes
2024-07-02 12:11:15,959:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:15,961:INFO:Initializing create_model()
2024-07-02 12:11:15,961:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:15,961:INFO:Checking exceptions
2024-07-02 12:11:15,962:INFO:Importing libraries
2024-07-02 12:11:15,962:INFO:Copying training dataset
2024-07-02 12:11:16,048:INFO:Defining folds
2024-07-02 12:11:16,048:INFO:Declaring metric variables
2024-07-02 12:11:16,062:INFO:Importing untrained model
2024-07-02 12:11:16,083:INFO:Random Forest Regressor Imported successfully
2024-07-02 12:11:16,108:INFO:Starting cross validation
2024-07-02 12:11:16,113:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:45,556:INFO:Calculating mean and std
2024-07-02 12:11:45,561:INFO:Creating metrics dataframe
2024-07-02 12:11:45,596:INFO:Uploading results into container
2024-07-02 12:11:45,597:INFO:Uploading model into container now
2024-07-02 12:11:45,598:INFO:_master_model_container: 2
2024-07-02 12:11:45,598:INFO:_display_container: 2
2024-07-02 12:11:45,604:INFO:RandomForestRegressor(n_jobs=-1, random_state=2570)
2024-07-02 12:11:45,604:INFO:create_model() successfully completed......................................
2024-07-02 12:11:45,833:INFO:SubProcess create_model() end ==================================
2024-07-02 12:11:45,842:INFO:Creating metrics dataframe
2024-07-02 12:11:45,865:INFO:Initializing Extra Trees Regressor
2024-07-02 12:11:45,865:INFO:Total runtime is 0.9499641021092733 minutes
2024-07-02 12:11:45,879:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:45,880:INFO:Initializing create_model()
2024-07-02 12:11:45,881:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:45,881:INFO:Checking exceptions
2024-07-02 12:11:45,882:INFO:Importing libraries
2024-07-02 12:11:45,882:INFO:Copying training dataset
2024-07-02 12:11:45,930:INFO:Defining folds
2024-07-02 12:11:45,930:INFO:Declaring metric variables
2024-07-02 12:11:45,940:INFO:Importing untrained model
2024-07-02 12:11:45,948:INFO:Extra Trees Regressor Imported successfully
2024-07-02 12:11:45,964:INFO:Starting cross validation
2024-07-02 12:11:45,982:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:46,502:WARNING:create_model() for et raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-02 12:11:46,510:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_forest.py", line 377, in fit
    estimator._compute_missing_values_in_feature_mask(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\tree\_classes.py", line 214, in _compute_missing_values_in_feature_mask
    assert_all_finite(X, **common_kwargs)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 216, in assert_all_finite
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
ExtraTreesRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:46,512:INFO:Initializing create_model()
2024-07-02 12:11:46,512:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:46,512:INFO:Checking exceptions
2024-07-02 12:11:46,512:INFO:Importing libraries
2024-07-02 12:11:46,512:INFO:Copying training dataset
2024-07-02 12:11:46,577:INFO:Defining folds
2024-07-02 12:11:46,578:INFO:Declaring metric variables
2024-07-02 12:11:46,588:INFO:Importing untrained model
2024-07-02 12:11:46,602:INFO:Extra Trees Regressor Imported successfully
2024-07-02 12:11:46,625:INFO:Starting cross validation
2024-07-02 12:11:46,628:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:46,996:ERROR:create_model() for et raised an exception or returned all 0.0:
2024-07-02 12:11:47,005:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_forest.py", line 377, in fit
    estimator._compute_missing_values_in_feature_mask(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\tree\_classes.py", line 214, in _compute_missing_values_in_feature_mask
    assert_all_finite(X, **common_kwargs)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 216, in assert_all_finite
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
ExtraTreesRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_forest.py", line 377, in fit
    estimator._compute_missing_values_in_feature_mask(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\tree\_classes.py", line 214, in _compute_missing_values_in_feature_mask
    assert_all_finite(X, **common_kwargs)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 216, in assert_all_finite
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
ExtraTreesRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:47,006:INFO:Initializing AdaBoost Regressor
2024-07-02 12:11:47,006:INFO:Total runtime is 0.9689823706944783 minutes
2024-07-02 12:11:47,011:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:47,012:INFO:Initializing create_model()
2024-07-02 12:11:47,012:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:47,012:INFO:Checking exceptions
2024-07-02 12:11:47,013:INFO:Importing libraries
2024-07-02 12:11:47,013:INFO:Copying training dataset
2024-07-02 12:11:47,111:INFO:Defining folds
2024-07-02 12:11:47,113:INFO:Declaring metric variables
2024-07-02 12:11:47,130:INFO:Importing untrained model
2024-07-02 12:11:47,145:INFO:AdaBoost Regressor Imported successfully
2024-07-02 12:11:47,172:INFO:Starting cross validation
2024-07-02 12:11:47,179:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:47,539:WARNING:create_model() for ada raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-02 12:11:47,542:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:47,542:INFO:Initializing create_model()
2024-07-02 12:11:47,542:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:47,542:INFO:Checking exceptions
2024-07-02 12:11:47,543:INFO:Importing libraries
2024-07-02 12:11:47,543:INFO:Copying training dataset
2024-07-02 12:11:47,598:INFO:Defining folds
2024-07-02 12:11:47,601:INFO:Declaring metric variables
2024-07-02 12:11:47,610:INFO:Importing untrained model
2024-07-02 12:11:47,640:INFO:AdaBoost Regressor Imported successfully
2024-07-02 12:11:47,655:INFO:Starting cross validation
2024-07-02 12:11:47,657:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:47,979:ERROR:create_model() for ada raised an exception or returned all 0.0:
2024-07-02 12:11:47,993:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:47,994:INFO:Initializing Gradient Boosting Regressor
2024-07-02 12:11:47,994:INFO:Total runtime is 0.985446031888326 minutes
2024-07-02 12:11:47,998:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:48,000:INFO:Initializing create_model()
2024-07-02 12:11:48,000:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:48,000:INFO:Checking exceptions
2024-07-02 12:11:48,000:INFO:Importing libraries
2024-07-02 12:11:48,000:INFO:Copying training dataset
2024-07-02 12:11:48,070:INFO:Defining folds
2024-07-02 12:11:48,073:INFO:Declaring metric variables
2024-07-02 12:11:48,097:INFO:Importing untrained model
2024-07-02 12:11:48,140:INFO:Gradient Boosting Regressor Imported successfully
2024-07-02 12:11:48,164:INFO:Starting cross validation
2024-07-02 12:11:48,170:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:48,587:WARNING:create_model() for gbr raised an exception or returned all 0.0, trying without fit_kwargs:
2024-07-02 12:11:48,588:WARNING:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_gb.py", line 659, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
GradientBoostingRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:48,589:INFO:Initializing create_model()
2024-07-02 12:11:48,589:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:48,589:INFO:Checking exceptions
2024-07-02 12:11:48,589:INFO:Importing libraries
2024-07-02 12:11:48,589:INFO:Copying training dataset
2024-07-02 12:11:48,661:INFO:Defining folds
2024-07-02 12:11:48,661:INFO:Declaring metric variables
2024-07-02 12:11:48,687:INFO:Importing untrained model
2024-07-02 12:11:48,710:INFO:Gradient Boosting Regressor Imported successfully
2024-07-02 12:11:48,755:INFO:Starting cross validation
2024-07-02 12:11:48,760:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:49,152:ERROR:create_model() for gbr raised an exception or returned all 0.0:
2024-07-02 12:11:49,155:ERROR:Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_gb.py", line 659, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
GradientBoostingRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 815, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1533, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1126, in _create_model_with_cv
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 450, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 536, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\ensemble\_gb.py", line 659, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1263, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
GradientBoostingRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


2024-07-02 12:11:49,155:INFO:Initializing Extreme Gradient Boosting
2024-07-02 12:11:49,155:INFO:Total runtime is 1.0047972559928895 minutes
2024-07-02 12:11:49,170:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:49,171:INFO:Initializing create_model()
2024-07-02 12:11:49,171:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:49,172:INFO:Checking exceptions
2024-07-02 12:11:49,172:INFO:Importing libraries
2024-07-02 12:11:49,172:INFO:Copying training dataset
2024-07-02 12:11:49,227:INFO:Defining folds
2024-07-02 12:11:49,227:INFO:Declaring metric variables
2024-07-02 12:11:49,235:INFO:Importing untrained model
2024-07-02 12:11:49,243:INFO:Extreme Gradient Boosting Imported successfully
2024-07-02 12:11:49,278:INFO:Starting cross validation
2024-07-02 12:11:49,285:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:11:55,781:INFO:Calculating mean and std
2024-07-02 12:11:55,783:INFO:Creating metrics dataframe
2024-07-02 12:11:55,786:INFO:Uploading results into container
2024-07-02 12:11:55,787:INFO:Uploading model into container now
2024-07-02 12:11:55,789:INFO:_master_model_container: 3
2024-07-02 12:11:55,800:INFO:_display_container: 2
2024-07-02 12:11:55,809:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=2570, ...)
2024-07-02 12:11:55,810:INFO:create_model() successfully completed......................................
2024-07-02 12:11:55,963:INFO:SubProcess create_model() end ==================================
2024-07-02 12:11:55,964:INFO:Creating metrics dataframe
2024-07-02 12:11:55,980:INFO:Initializing Light Gradient Boosting Machine
2024-07-02 12:11:55,980:INFO:Total runtime is 1.1185548504193625 minutes
2024-07-02 12:11:55,991:INFO:SubProcess create_model() called ==================================
2024-07-02 12:11:55,992:INFO:Initializing create_model()
2024-07-02 12:11:55,993:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:11:55,993:INFO:Checking exceptions
2024-07-02 12:11:55,993:INFO:Importing libraries
2024-07-02 12:11:55,993:INFO:Copying training dataset
2024-07-02 12:11:56,052:INFO:Defining folds
2024-07-02 12:11:56,052:INFO:Declaring metric variables
2024-07-02 12:11:56,071:INFO:Importing untrained model
2024-07-02 12:11:56,086:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-02 12:11:56,121:INFO:Starting cross validation
2024-07-02 12:11:56,128:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:12:01,755:INFO:Calculating mean and std
2024-07-02 12:12:01,757:INFO:Creating metrics dataframe
2024-07-02 12:12:01,765:INFO:Uploading results into container
2024-07-02 12:12:01,770:INFO:Uploading model into container now
2024-07-02 12:12:01,771:INFO:_master_model_container: 4
2024-07-02 12:12:01,772:INFO:_display_container: 2
2024-07-02 12:12:01,773:INFO:LGBMRegressor(n_jobs=-1, random_state=2570)
2024-07-02 12:12:01,774:INFO:create_model() successfully completed......................................
2024-07-02 12:12:01,934:INFO:SubProcess create_model() end ==================================
2024-07-02 12:12:01,935:INFO:Creating metrics dataframe
2024-07-02 12:12:01,949:INFO:Initializing CatBoost Regressor
2024-07-02 12:12:01,951:INFO:Total runtime is 1.2180622657140097 minutes
2024-07-02 12:12:01,960:INFO:SubProcess create_model() called ==================================
2024-07-02 12:12:01,961:INFO:Initializing create_model()
2024-07-02 12:12:01,961:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:12:01,961:INFO:Checking exceptions
2024-07-02 12:12:01,961:INFO:Importing libraries
2024-07-02 12:12:01,962:INFO:Copying training dataset
2024-07-02 12:12:02,031:INFO:Defining folds
2024-07-02 12:12:02,041:INFO:Declaring metric variables
2024-07-02 12:12:02,127:INFO:Importing untrained model
2024-07-02 12:12:02,186:INFO:CatBoost Regressor Imported successfully
2024-07-02 12:12:02,220:INFO:Starting cross validation
2024-07-02 12:12:02,223:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:13:41,123:INFO:Calculating mean and std
2024-07-02 12:13:41,128:INFO:Creating metrics dataframe
2024-07-02 12:13:41,164:INFO:Uploading results into container
2024-07-02 12:13:41,165:INFO:Uploading model into container now
2024-07-02 12:13:41,167:INFO:_master_model_container: 5
2024-07-02 12:13:41,167:INFO:_display_container: 2
2024-07-02 12:13:41,167:INFO:<catboost.core.CatBoostRegressor object at 0x0000016E6C3B3C90>
2024-07-02 12:13:41,167:INFO:create_model() successfully completed......................................
2024-07-02 12:13:41,432:INFO:SubProcess create_model() end ==================================
2024-07-02 12:13:41,432:INFO:Creating metrics dataframe
2024-07-02 12:13:41,459:INFO:Initializing Dummy Regressor
2024-07-02 12:13:41,460:INFO:Total runtime is 2.876543136437734 minutes
2024-07-02 12:13:41,476:INFO:SubProcess create_model() called ==================================
2024-07-02 12:13:41,477:INFO:Initializing create_model()
2024-07-02 12:13:41,477:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016E6CEB1A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:13:41,477:INFO:Checking exceptions
2024-07-02 12:13:41,478:INFO:Importing libraries
2024-07-02 12:13:41,478:INFO:Copying training dataset
2024-07-02 12:13:41,616:INFO:Defining folds
2024-07-02 12:13:41,625:INFO:Declaring metric variables
2024-07-02 12:13:41,646:INFO:Importing untrained model
2024-07-02 12:13:41,767:INFO:Dummy Regressor Imported successfully
2024-07-02 12:13:41,827:INFO:Starting cross validation
2024-07-02 12:13:41,829:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:13:42,316:INFO:Calculating mean and std
2024-07-02 12:13:42,325:INFO:Creating metrics dataframe
2024-07-02 12:13:42,331:INFO:Uploading results into container
2024-07-02 12:13:42,333:INFO:Uploading model into container now
2024-07-02 12:13:42,335:INFO:_master_model_container: 6
2024-07-02 12:13:42,336:INFO:_display_container: 2
2024-07-02 12:13:42,337:INFO:DummyRegressor()
2024-07-02 12:13:42,340:INFO:create_model() successfully completed......................................
2024-07-02 12:13:42,579:INFO:SubProcess create_model() end ==================================
2024-07-02 12:13:42,579:INFO:Creating metrics dataframe
2024-07-02 12:13:42,624:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-07-02 12:13:42,675:INFO:Initializing create_model()
2024-07-02 12:13:42,676:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=<catboost.core.CatBoostRegressor object at 0x0000016E6C3B3C90>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:13:42,676:INFO:Checking exceptions
2024-07-02 12:13:42,688:INFO:Importing libraries
2024-07-02 12:13:42,688:INFO:Copying training dataset
2024-07-02 12:13:42,789:INFO:Defining folds
2024-07-02 12:13:42,790:INFO:Declaring metric variables
2024-07-02 12:13:42,790:INFO:Importing untrained model
2024-07-02 12:13:42,790:INFO:Declaring custom model
2024-07-02 12:13:42,792:INFO:CatBoost Regressor Imported successfully
2024-07-02 12:13:42,794:INFO:Cross validation set to False
2024-07-02 12:13:42,794:INFO:Fitting Model
2024-07-02 12:14:03,318:INFO:<catboost.core.CatBoostRegressor object at 0x0000016E6CDDE9D0>
2024-07-02 12:14:03,318:INFO:create_model() successfully completed......................................
2024-07-02 12:14:03,719:INFO:_master_model_container: 6
2024-07-02 12:14:03,719:INFO:_display_container: 2
2024-07-02 12:14:03,719:INFO:<catboost.core.CatBoostRegressor object at 0x0000016E6CDDE9D0>
2024-07-02 12:14:03,719:INFO:compare_models() successfully completed......................................
2024-07-02 12:16:08,358:INFO:Initializing create_model()
2024-07-02 12:16:08,359:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=catboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-02 12:16:08,359:INFO:Checking exceptions
2024-07-02 12:16:08,388:INFO:Importing libraries
2024-07-02 12:16:08,389:INFO:Copying training dataset
2024-07-02 12:16:08,460:INFO:Defining folds
2024-07-02 12:16:08,462:INFO:Declaring metric variables
2024-07-02 12:16:08,482:INFO:Importing untrained model
2024-07-02 12:16:08,497:INFO:CatBoost Regressor Imported successfully
2024-07-02 12:16:08,563:INFO:Starting cross validation
2024-07-02 12:16:08,566:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-02 12:17:33,856:INFO:Calculating mean and std
2024-07-02 12:17:33,859:INFO:Creating metrics dataframe
2024-07-02 12:17:33,873:INFO:Finalizing model
2024-07-02 12:18:13,415:INFO:Uploading results into container
2024-07-02 12:18:13,416:INFO:Uploading model into container now
2024-07-02 12:18:13,470:INFO:_master_model_container: 7
2024-07-02 12:18:13,470:INFO:_display_container: 3
2024-07-02 12:18:13,471:INFO:<catboost.core.CatBoostRegressor object at 0x0000016E6C563910>
2024-07-02 12:18:13,471:INFO:create_model() successfully completed......................................
2024-07-02 12:18:22,206:INFO:Initializing evaluate_model()
2024-07-02 12:18:22,206:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=<catboost.core.CatBoostRegressor object at 0x0000016E6C563910>, fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-07-02 12:18:22,249:INFO:Initializing plot_model()
2024-07-02 12:18:22,249:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=<catboost.core.CatBoostRegressor object at 0x0000016E6C563910>, plot=pipeline, scale=1, save=False, fold=KFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-07-02 12:18:22,250:INFO:Checking exceptions
2024-07-02 12:18:22,270:INFO:Preloading libraries
2024-07-02 12:18:22,349:INFO:Copying training dataset
2024-07-02 12:18:22,349:INFO:Plot type: pipeline
2024-07-02 12:18:23,857:INFO:Visual Rendered Successfully
2024-07-02 12:18:24,177:INFO:plot_model() successfully completed......................................
2024-07-02 12:18:56,757:INFO:Initializing plot_model()
2024-07-02 12:18:56,757:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=<catboost.core.CatBoostRegressor object at 0x0000016E6C563910>, plot=feature, scale=1, save=False, fold=KFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-07-02 12:18:56,757:INFO:Checking exceptions
2024-07-02 12:18:56,775:INFO:Preloading libraries
2024-07-02 12:18:56,783:INFO:Copying training dataset
2024-07-02 12:18:56,783:INFO:Plot type: feature
2024-07-02 12:18:56,784:WARNING:No coef_ found. Trying feature_importances_
2024-07-02 12:18:57,832:INFO:Visual Rendered Successfully
2024-07-02 12:18:58,067:INFO:plot_model() successfully completed......................................
2024-07-02 12:19:19,403:INFO:Initializing plot_model()
2024-07-02 12:19:19,403:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=<catboost.core.CatBoostRegressor object at 0x0000016E6C563910>, plot=feature_all, scale=1, save=False, fold=KFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-07-02 12:19:19,403:INFO:Checking exceptions
2024-07-02 12:19:19,421:INFO:Preloading libraries
2024-07-02 12:19:19,428:INFO:Copying training dataset
2024-07-02 12:19:19,429:INFO:Plot type: feature_all
2024-07-02 12:19:19,561:WARNING:No coef_ found. Trying feature_importances_
2024-07-02 12:19:20,819:INFO:Visual Rendered Successfully
2024-07-02 12:19:21,005:INFO:plot_model() successfully completed......................................
2024-07-02 12:20:23,588:INFO:Initializing plot_model()
2024-07-02 12:20:23,588:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=<catboost.core.CatBoostRegressor object at 0x0000016E6C563910>, plot=learning, scale=1, save=False, fold=KFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-07-02 12:20:23,588:INFO:Checking exceptions
2024-07-02 12:20:23,610:INFO:Preloading libraries
2024-07-02 12:20:23,622:INFO:Copying training dataset
2024-07-02 12:20:23,622:INFO:Plot type: learning
2024-07-02 12:20:25,453:INFO:Fitting Model
2024-07-02 12:23:04,865:INFO:Initializing predict_model()
2024-07-02 12:23:04,865:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=<catboost.core.CatBoostRegressor object at 0x0000016E6C563910>, probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000016E6F33CD60>)
2024-07-02 12:23:04,866:INFO:Checking exceptions
2024-07-02 12:23:04,866:INFO:Preloading libraries
2024-07-02 12:23:05,279:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2024-07-02 12:23:05,603:INFO:Initializing predict_model()
2024-07-02 12:23:05,603:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=<catboost.core.CatBoostRegressor object at 0x0000016E6C563910>, probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000016E6CE5A660>)
2024-07-02 12:23:05,604:INFO:Checking exceptions
2024-07-02 12:23:05,604:INFO:Preloading libraries
2024-07-02 12:23:05,970:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2024-07-02 12:23:23,948:INFO:Initializing evaluate_model()
2024-07-02 12:23:23,948:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=<catboost.core.CatBoostRegressor object at 0x0000016E6C563910>, fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-07-02 12:23:23,980:INFO:Initializing plot_model()
2024-07-02 12:23:23,980:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=<catboost.core.CatBoostRegressor object at 0x0000016E6C563910>, plot=pipeline, scale=1, save=False, fold=KFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-07-02 12:23:23,980:INFO:Checking exceptions
2024-07-02 12:23:24,004:INFO:Preloading libraries
2024-07-02 12:23:24,013:INFO:Copying training dataset
2024-07-02 12:23:24,013:INFO:Plot type: pipeline
2024-07-02 12:23:24,458:INFO:Visual Rendered Successfully
2024-07-02 12:23:24,715:INFO:plot_model() successfully completed......................................
2024-07-02 12:23:27,941:INFO:Initializing predict_model()
2024-07-02 12:23:27,941:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=<catboost.core.CatBoostRegressor object at 0x0000016E6C563910>, probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000016E6A79A3E0>)
2024-07-02 12:23:27,941:INFO:Checking exceptions
2024-07-02 12:23:27,942:INFO:Preloading libraries
2024-07-02 12:23:28,230:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2024-07-02 12:23:41,394:INFO:Initializing predict_model()
2024-07-02 12:23:41,394:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016E6492A090>, estimator=<catboost.core.CatBoostRegressor object at 0x0000016E6C563910>, probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000016E6C8F7100>)
2024-07-02 12:23:41,395:INFO:Checking exceptions
2024-07-02 12:23:41,395:INFO:Preloading libraries
2024-07-02 12:23:41,398:INFO:Set up data.
2024-07-02 12:23:41,793:INFO:Set up index.
2024-07-02 12:23:42,259:WARNING:e:\Python Projects\F1RaceOutcome\F1RaceOutcomeEnv\Lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

